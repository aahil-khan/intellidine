Attaching to intellidine-analytics-service, intellidine-api-gateway, intellidine-auth-service, intellidine-discount-engine, intellidine-grafana, intellidine-inventory-service, intellidine-kafka, intellidine-menu-service, intellidine-ml-service, intellidine-nginx, intellidine-notification-service, intellidine-order-service, intellidine-payment-service, intellidine-postgres, intellidine-prometheus, intellidine-redis, intellidine-zookeeper
intellidine-zookeeper   | ===> User
intellidine-zookeeper   | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
intellidine-zookeeper   | ===> Configuring ...
intellidine-postgres    | 
intellidine-postgres    | PostgreSQL Database directory appears to contain a database; Skipping initialization
intellidine-postgres    | 
intellidine-redis       | 1:C 21 Oct 2025 17:27:11.915 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see https://github.com/jemalloc/jemalloc/issues/1328. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
intellidine-redis       | 1:C 21 Oct 2025 17:27:11.915 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
intellidine-redis       | 1:C 21 Oct 2025 17:27:11.915 * Redis version=7.4.5, bits=64, commit=00000000, modified=0, pid=1, just started
intellidine-redis       | 1:C 21 Oct 2025 17:27:11.915 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
intellidine-redis       | 1:M 21 Oct 2025 17:27:11.917 * monotonic clock: POSIX clock_gettime
intellidine-redis       | 1:M 21 Oct 2025 17:27:11.927 * Running mode=standalone, port=6379.
intellidine-redis       | 1:M 21 Oct 2025 17:27:11.928 * Server initialized
intellidine-redis       | 1:M 21 Oct 2025 17:27:11.933 * Loading RDB produced by version 7.4.5
intellidine-redis       | 1:M 21 Oct 2025 17:27:11.934 * RDB age 247 seconds
intellidine-redis       | 1:M 21 Oct 2025 17:27:11.934 * RDB memory usage when created 0.90 Mb
intellidine-redis       | 1:M 21 Oct 2025 17:27:11.935 * Done loading RDB, keys loaded: 0, keys expired: 0.
intellidine-redis       | 1:M 21 Oct 2025 17:27:11.935 * DB loaded from disk: 0.004 seconds
intellidine-redis       | 1:M 21 Oct 2025 17:27:11.936 * Ready to accept connections tcp
intellidine-kafka       | ===> User
intellidine-kafka       | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
intellidine-kafka       | ===> Configuring ...
intellidine-kafka       | Running in Zookeeper mode...
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.045310848Z level=info msg="Starting Grafana" version=12.2.0 commit=92f1fba9b4b6700328e99e97328d6639df8ddc3d branch=release-12.2.0 compiled=2025-10-21T17:27:13Z
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052023182Z level=info msg="Config loaded from" file=/usr/share/grafana/conf/defaults.ini
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052186737Z level=info msg="Config loaded from" file=/etc/grafana/grafana.ini
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052278657Z level=info msg="Config overridden from command line" arg="default.paths.data=/var/lib/grafana"
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052338465Z level=info msg="Config overridden from command line" arg="default.paths.logs=/var/log/grafana"
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.05240089Z level=info msg="Config overridden from command line" arg="default.paths.plugins=/var/lib/grafana/plugins"
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052453813Z level=info msg="Config overridden from command line" arg="default.paths.provisioning=/etc/grafana/provisioning"
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052515168Z level=info msg="Config overridden from command line" arg="default.log.mode=console"
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052569487Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_DATA=/var/lib/grafana"
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.05263224Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_LOGS=/var/log/grafana"
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052695645Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_PLUGINS=/var/lib/grafana/plugins"
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052766427Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_PROVISIONING=/etc/grafana/provisioning"
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052821163Z level=info msg="Config overridden from Environment variable" var="GF_SECURITY_ADMIN_PASSWORD=*********"
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052843741Z level=info msg=Target target=[all]
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052894134Z level=info msg="Path Home" path=/usr/share/grafana
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052910562Z level=info msg="Path Data" path=/var/lib/grafana
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052926859Z level=info msg="Path Logs" path=/var/log/grafana
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052941879Z level=info msg="Path Plugins" path=/var/lib/grafana/plugins
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052957496Z level=info msg="Path Provisioning" path=/etc/grafana/provisioning
intellidine-grafana     | logger=settings t=2025-10-21T17:27:13.052973476Z level=info msg="App mode production"
intellidine-grafana     | logger=featuremgmt t=2025-10-21T17:27:13.055268245Z level=info msg=FeatureToggles newDashboardSharingComponent=true alertingRulePermanentlyDelete=true logsInfiniteScrolling=true kubernetesDashboards=true newFiltersUI=true dashboardSceneSolo=true logsExploreTableVisualisation=true alertingNotificationsStepMode=true annotationPermissionUpdate=true alertingBulkActionsInUI=true cloudWatchCrossAccountQuerying=true correlations=true preinstallAutoUpdate=true improvedExternalSessionHandlingSAML=true skipTokenRotationIfRecent=true groupToNestedTableTransformation=true cloudWatchNewLabelParsing=true unifiedStorageHistoryPruner=true tlsMemcached=true ssoSettingsLDAP=true unifiedRequestLog=true publicDashboardsScene=true alertingRuleVersionHistoryRestore=true alertingRuleRecoverDeleted=true awsAsyncQueryCaching=true alertingImportYAMLUI=true awsDatasourcesTempCredentials=true addFieldFromCalculationStatFunctions=true azureMonitorPrometheusExemplars=true alertingUIOptimizeReducer=true logsContextDatasourceUi=true influxdbBackendMigration=true dashboardSceneForViewers=true useSessionStorageForRedirection=true newPDFRendering=true panelMonitoring=true onPremToCloudMigrations=true grafanaconThemes=true grafanaAssistantInProfilesDrilldown=true alertingMigrationUI=true improvedExternalSessionHandling=true promQLScope=true alertingQueryAndExpressionsStepMode=true alertingSaveStateCompressed=true dataplaneFrontendFallback=true dashgpt=true adhocFiltersInTooltips=true formatString=true lokiQuerySplitting=true cloudWatchRoundUpEndTime=true prometheusAzureOverrideAudience=true dashboardDsAdHocFiltering=true logRowsPopoverMenu=true recordedQueriesMulti=true alertRuleRestore=true lokiLabelNamesQueryApi=true logsPanelControls=true dashboardScene=true transformationsRedesign=true azureMonitorEnableUserAuth=true pinNavItems=true
intellidine-grafana     | logger=sqlstore t=2025-10-21T17:27:13.055752493Z level=info msg="Connecting to DB" dbtype=sqlite3
intellidine-grafana     | logger=migrator t=2025-10-21T17:27:13.097133722Z level=info msg="Locking database"
intellidine-grafana     | logger=migrator t=2025-10-21T17:27:13.097877366Z level=info msg="Starting DB migrations"
intellidine-grafana     | logger=migrator t=2025-10-21T17:27:13.191460521Z level=info msg="migrations completed" performed=0 skipped=674 duration=2.583694ms
intellidine-grafana     | logger=migrator t=2025-10-21T17:27:13.193905239Z level=info msg="Unlocking database"
intellidine-grafana     | logger=secrets t=2025-10-21T17:27:13.196021796Z level=info msg="Envelope encryption state" enabled=true currentprovider=secretKey.v1
intellidine-discount-engine  | [32m[Nest] 1  - [39m10/21/2025, 5:27:13 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-discount-engine  | [32m[Nest] 1  - [39m10/21/2025, 5:27:13 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +43ms[39m
intellidine-discount-engine  | [32m[Nest] 1  - [39m10/21/2025, 5:27:13 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/discount}:[39m[38;5;3m +17ms[39m
intellidine-discount-engine  | [32m[Nest] 1  - [39m10/21/2025, 5:27:13 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/discount/health, GET} route[39m[38;5;3m +10ms[39m
intellidine-discount-engine  | [32m[Nest] 1  - [39m10/21/2025, 5:27:13 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/discount/evaluate, POST} route[39m[38;5;3m +3ms[39m
intellidine-discount-engine  | [32m[Nest] 1  - [39m10/21/2025, 5:27:13 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/discount/rules, GET} route[39m[38;5;3m +2ms[39m
intellidine-discount-engine  | [32m[Nest] 1  - [39m10/21/2025, 5:27:13 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/discount/rules, POST} route[39m[38;5;3m +2ms[39m
intellidine-discount-engine  | [32m[Nest] 1  - [39m10/21/2025, 5:27:13 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/discount/templates, GET} route[39m[38;5;3m +1ms[39m
intellidine-discount-engine  | [32m[Nest] 1  - [39m10/21/2025, 5:27:13 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/discount/simulate, POST} route[39m[38;5;3m +1ms[39m
intellidine-discount-engine  | [32m[Nest] 1  - [39m10/21/2025, 5:27:13 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/discount/stats, GET} route[39m[38;5;3m +3ms[39m
intellidine-discount-engine  | [32m[Nest] 1  - [39m10/21/2025, 5:27:13 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +7ms[39m
intellidine-grafana          | logger=plugin.angulardetectorsprovider.dynamic t=2025-10-21T17:27:13.628460788Z level=info msg="Restored cache from database" duration=1.230925ms
intellidine-grafana          | logger=resource-db t=2025-10-21T17:27:13.632991166Z level=info msg="Using database section" db_type=sqlite3
intellidine-grafana          | logger=resource-db t=2025-10-21T17:27:13.633360262Z level=info msg="Initializing Resource DB" db_type=sqlite3 open_conn=0 in_use_conn=0 idle_conn=0 max_open_conn=0
intellidine-grafana          | logger=resource-migrator t=2025-10-21T17:27:13.663872536Z level=info msg="Locking database"
intellidine-grafana          | logger=resource-migrator t=2025-10-21T17:27:13.664035853Z level=info msg="Starting DB migrations"
intellidine-grafana          | logger=resource-migrator t=2025-10-21T17:27:13.700376515Z level=info msg="migrations completed" performed=0 skipped=26 duration=141.054Âµs
intellidine-grafana          | logger=resource-migrator t=2025-10-21T17:27:13.702155911Z level=info msg="Unlocking database"
intellidine-grafana          | t=2025-10-21T17:27:13.702944518Z level=info caller=logger.go:214 time=2025-10-21T17:27:13.702901761Z msg="Using channel notifier" logger=sql-resource-server
intellidine-grafana          | logger=plugin.store t=2025-10-21T17:27:13.706290293Z level=info msg="Loading plugins..."
intellidine-grafana          | logger=plugins.registration t=2025-10-21T17:27:14.071813569Z level=info msg="Plugin registered" pluginId=grafana-exploretraces-app
intellidine-grafana          | logger=plugins.registration t=2025-10-21T17:27:14.31389718Z level=info msg="Plugin registered" pluginId=grafana-lokiexplore-app
intellidine-grafana          | logger=plugins.registration t=2025-10-21T17:27:14.457060944Z level=info msg="Plugin registered" pluginId=grafana-metricsdrilldown-app
intellidine-grafana          | logger=plugins.registration t=2025-10-21T17:27:14.576596231Z level=info msg="Plugin registered" pluginId=grafana-pyroscope-app
intellidine-grafana          | logger=plugin.store t=2025-10-21T17:27:14.576791763Z level=info msg="Plugins loaded" count=56 duration=870.50518ms
intellidine-grafana          | logger=query_data t=2025-10-21T17:27:14.5973955Z level=info msg="Query Service initialization"
intellidine-grafana          | logger=live.push_http t=2025-10-21T17:27:14.63319516Z level=info msg="Live Push Gateway initialization"
intellidine-grafana          | logger=ngalert.notifier component=alertmanager orgID=1 t=2025-10-21T17:27:14.650202274Z level=info msg="Applying new configuration to Alertmanager" configHash=d2c56faca6af2a5772ff4253222f7386
intellidine-grafana          | logger=ngalert.writer t=2025-10-21T17:27:14.695944652Z level=info msg="Setting up remote write using data sources" timeout=30s default_datasource_uid=
intellidine-grafana          | logger=ngalert t=2025-10-21T17:27:14.696082284Z level=info msg="Using protobuf-based alert instance store"
intellidine-grafana          | logger=ngalert.state.manager.persist t=2025-10-21T17:27:14.696130484Z level=info msg="Using rule state persister"
intellidine-grafana          | logger=secret-migrator t=2025-10-21T17:27:14.703106226Z level=info msg="Locking database"
intellidine-grafana          | logger=secret-migrator t=2025-10-21T17:27:14.703147983Z level=info msg="Starting DB migrations"
intellidine-grafana          | logger=secret-migrator t=2025-10-21T17:27:14.711430254Z level=info msg="migrations completed" performed=0 skipped=24 duration=129.629Âµs
intellidine-grafana          | logger=secret-migrator t=2025-10-21T17:27:14.715177649Z level=info msg="Unlocking database"
intellidine-grafana          | logger=infra.usagestats.collector t=2025-10-21T17:27:14.74336918Z level=info msg="registering usage stat providers" usageStatsProvidersLen=2
intellidine-grafana          | logger=provisioning.alerting t=2025-10-21T17:27:14.74728352Z level=info msg="starting to provision alerting"
intellidine-grafana          | logger=provisioning.alerting t=2025-10-21T17:27:14.747500568Z level=info msg="finished to provision alerting"
intellidine-grafana          | logger=grafanaStorageLogger t=2025-10-21T17:27:14.748528823Z level=info msg="Storage starting"
intellidine-grafana          | logger=provisioning.dashboard t=2025-10-21T17:27:14.74900351Z level=info msg="starting to provision dashboards"
intellidine-grafana          | logger=provisioning.dashboard t=2025-10-21T17:27:14.749172969Z level=info msg="finished to provision dashboards"
intellidine-grafana          | logger=ngalert.state.manager t=2025-10-21T17:27:14.750947223Z level=info msg="Warming state cache for startup"
intellidine-grafana          | logger=ngalert.multiorg.alertmanager t=2025-10-21T17:27:14.752831906Z level=info msg="Starting MultiOrg Alertmanager"
intellidine-grafana          | logger=http.server t=2025-10-21T17:27:14.777515929Z level=info msg="HTTP Server Listen" address=[::]:3000 protocol=http subUrl= socket=
intellidine-grafana          | logger=ngalert.state.manager t=2025-10-21T17:27:15.018138715Z level=info msg="State cache has been initialized" states=0 duration=267.186359ms
intellidine-grafana          | logger=ngalert.scheduler t=2025-10-21T17:27:15.019184957Z level=info msg="Starting scheduler" tickInterval=10s maxAttempts=3
intellidine-grafana          | logger=ngalert.scheduler t=2025-10-21T17:27:15.020028748Z level=info msg=starting component=ticker first_tick=2025-10-21T17:27:20Z
intellidine-analytics-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +67ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/analytics}:[39m[38;5;3m +27ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/health, GET} route[39m[38;5;3m +17ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/daily, GET} route[39m[38;5;3m +2ms[39m
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +55ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/recent, GET} route[39m[38;5;3m +3ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/range, GET} route[39m[38;5;3m +2ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/aggregated, GET} route[39m[38;5;3m +2ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/orders/history, GET} route[39m[38;5;3m +1ms[39m
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[WebSocketsController] [39m[32mNotificationGateway subscribed to the "subscribe:orders" message[39m[38;5;3m +49ms[39m
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[WebSocketsController] [39m[32mNotificationGateway subscribed to the "unsubscribe:orders" message[39m[38;5;3m +0ms[39m
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[WebSocketsController] [39m[32mNotificationGateway subscribed to the "subscribe:kitchen" message[39m[38;5;3m +1ms[39m
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[WebSocketsController] [39m[32mNotificationGateway subscribed to the "unsubscribe:kitchen" message[39m[38;5;3m +1ms[39m
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[WebSocketsController] [39m[32mNotificationGateway subscribed to the "subscribe:managers" message[39m[38;5;3m +0ms[39m
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[WebSocketsController] [39m[32mNotificationGateway subscribed to the "unsubscribe:managers" message[39m[38;5;3m +0ms[39m
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[WebSocketsController] [39m[32mNotificationGateway subscribed to the "ping" message[39m[38;5;3m +0ms[39m
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +5ms[39m
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +7ms[39m
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/notifications/stats, GET} route[39m[38;5;3m +1ms[39m
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:15 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/notifications/test, GET} route[39m[38;5;3m +2ms[39m
intellidine-grafana               | logger=plugins.update.checker t=2025-10-21T17:27:15.722994752Z level=info msg="Update check succeeded" duration=976.060857ms
intellidine-grafana               | logger=grafana.update.checker t=2025-10-21T17:27:15.759798492Z level=info msg="Update check succeeded" duration=1.015036686s
[Kintellidine-analytics-service exited with code 1 (restarting)
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:17 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:17 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +34ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:17 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/analytics}:[39m[38;5;3m +14ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:17 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/health, GET} route[39m[38;5;3m +8ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:17 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/daily, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:17 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/recent, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:17 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/range, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:17 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/aggregated, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:17 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/orders/history, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:17.445Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.21.0.9:9092","broker":"kafka:9092","clientId":"analytics-service","stack":"Error: connect ECONNREFUSED 172.21.0.9:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1611:16)"}
intellidine-analytics-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:17.448Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.21.0.9:9092","retryCount":0,"retryTime":256}
intellidine-analytics-service     | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-analytics-service     | Please manually install OpenSSL and try installing Prisma again.
intellidine-analytics-service     | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-analytics-service     | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-analytics-service     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-analytics-service     | 
intellidine-analytics-service     | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-analytics-service     | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-analytics-service     | 
intellidine-analytics-service     | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-analytics-service     |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-analytics-service     |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-analytics-service     |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-analytics-service     |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-analytics-service     |     at async Proxy.onModuleInit (/app/dist/prisma.service.js:19:9)
intellidine-analytics-service     |     at async Promise.all (index 0)
intellidine-analytics-service     |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-analytics-service     |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-analytics-service     |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-analytics-service     |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-analytics-service     |   clientVersion: '5.22.0',
intellidine-analytics-service     |   errorCode: undefined
intellidine-analytics-service     | }
intellidine-analytics-service     | 
intellidine-analytics-service     | Node.js v20.19.5
[Kintellidine-analytics-service exited with code 1 (restarting)
intellidine-grafana               | logger=grafana-apiserver t=2025-10-21T17:27:18.187801341Z level=info msg="Adding GroupVersion notifications.alerting.grafana.app v0alpha1 to ResourceManager"
intellidine-grafana               | logger=grafana-apiserver t=2025-10-21T17:27:18.201217909Z level=info msg="Adding GroupVersion dashboard.grafana.app v1beta1 to ResourceManager"
intellidine-grafana               | logger=grafana-apiserver t=2025-10-21T17:27:18.202706464Z level=info msg="Adding GroupVersion dashboard.grafana.app v0alpha1 to ResourceManager"
intellidine-grafana               | logger=grafana-apiserver t=2025-10-21T17:27:18.204292842Z level=info msg="Adding GroupVersion dashboard.grafana.app v2beta1 to ResourceManager"
intellidine-grafana               | logger=grafana-apiserver t=2025-10-21T17:27:18.205842623Z level=info msg="Adding GroupVersion dashboard.grafana.app v2alpha1 to ResourceManager"
intellidine-grafana               | logger=grafana-apiserver t=2025-10-21T17:27:18.210056179Z level=info msg="Adding GroupVersion folder.grafana.app v1beta1 to ResourceManager"
intellidine-grafana               | logger=grafana-apiserver t=2025-10-21T17:27:18.216324663Z level=info msg="Adding GroupVersion iam.grafana.app v0alpha1 to ResourceManager"
intellidine-grafana               | logger=grafana-apiserver t=2025-10-21T17:27:18.218707801Z level=info msg="Adding GroupVersion userstorage.grafana.app v0alpha1 to ResourceManager"
intellidine-grafana               | logger=grafana-apiserver t=2025-10-21T17:27:18.220135117Z level=info msg="Adding GroupVersion features.grafana.app v0alpha1 to ResourceManager"
intellidine-grafana               | logger=grafana-apiserver t=2025-10-21T17:27:18.223112692Z level=info msg="Adding GroupVersion playlist.grafana.app v0alpha1 to ResourceManager"
intellidine-grafana               | t=2025-10-21T17:27:18.223443468Z level=info caller=logger.go:214 time=2025-10-21T17:27:18.223421702Z msg="Installed APIs for app" app=playlist
intellidine-grafana               | logger=grafana-apiserver t=2025-10-21T17:27:18.224139019Z level=info msg="Skipping API plugins.grafana.app/v0alpha1 because it has no resources."
intellidine-grafana               | t=2025-10-21T17:27:18.224668771Z level=info caller=logger.go:214 time=2025-10-21T17:27:18.224650775Z msg="Installed APIs for app" app=plugins
intellidine-grafana               | t=2025-10-21T17:27:18.416259907Z level=info caller=logger.go:214 time=2025-10-21T17:27:18.416244721Z msg="App initialized" app=playlist
intellidine-grafana               | t=2025-10-21T17:27:18.416353504Z level=info caller=logger.go:214 time=2025-10-21T17:27:18.416333195Z msg="App initialized" app=plugins
intellidine-grafana               | logger=app-registry t=2025-10-21T17:27:18.416670292Z level=info msg="app registry initialized"
intellidine-zookeeper             | ===> Running preflight checks ... 
intellidine-zookeeper             | ===> Check if /var/lib/zookeeper/data is writable ...
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:19 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:19 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +52ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:19 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/analytics}:[39m[38;5;3m +18ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:19 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/health, GET} route[39m[38;5;3m +11ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:19 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/daily, GET} route[39m[38;5;3m +2ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:19 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/recent, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:19 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/range, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:19 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/aggregated, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:19 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/orders/history, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:19.656Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.21.0.9:9092","broker":"kafka:9092","clientId":"analytics-service","stack":"Error: connect ECONNREFUSED 172.21.0.9:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1611:16)"}
intellidine-analytics-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:19.662Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.21.0.9:9092","retryCount":0,"retryTime":286}
intellidine-analytics-service     | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-analytics-service     | Please manually install OpenSSL and try installing Prisma again.
intellidine-analytics-service     | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-analytics-service     | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-analytics-service     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-analytics-service     | 
intellidine-analytics-service     | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-analytics-service     | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-analytics-service     | 
intellidine-analytics-service     | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-analytics-service     |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-analytics-service     |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-analytics-service     |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-analytics-service     |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-analytics-service     |     at async Proxy.onModuleInit (/app/dist/prisma.service.js:19:9)
intellidine-analytics-service     |     at async Promise.all (index 0)
intellidine-analytics-service     |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-analytics-service     |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-analytics-service     |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-analytics-service     |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-analytics-service     |   clientVersion: '5.22.0',
intellidine-analytics-service     |   errorCode: undefined
intellidine-analytics-service     | }
intellidine-analytics-service     | 
intellidine-analytics-service     | Node.js v20.19.5
intellidine-zookeeper             | ===> Check if /var/lib/zookeeper/log is writable ...
[Kintellidine-analytics-service exited with code 1 (restarting)
intellidine-notification-service  | [33m[Nest] 1  - [39m10/21/2025, 5:27:20 PM [33m   WARN[39m [38;5;3m[KafkaConsumerService] [39m[33mConsumer not connected, skipping subscription[39m
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:20 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +2ms[39m
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:20 PM [32m    LOG[39m [38;5;3m[NotificationService] [39m[32mNotification Service running on port 3106[39m
intellidine-zookeeper             | ===> Launching ... 
intellidine-zookeeper             | ===> Launching zookeeper ... 
intellidine-kafka                 | ===> Running preflight checks ... 
intellidine-kafka                 | ===> Check if /var/lib/kafka/data is writable ...
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:22 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:22 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +48ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:22 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/analytics}:[39m[38;5;3m +31ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:22 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/health, GET} route[39m[38;5;3m +9ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:22 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/daily, GET} route[39m[38;5;3m +2ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:22 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/recent, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:22 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/range, GET} route[39m[38;5;3m +2ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:22 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/aggregated, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:22 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/orders/history, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:22.180Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.21.0.9:9092","broker":"kafka:9092","clientId":"analytics-service","stack":"Error: connect ECONNREFUSED 172.21.0.9:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1611:16)"}
intellidine-analytics-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:22.187Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.21.0.9:9092","retryCount":0,"retryTime":334}
intellidine-analytics-service     | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-analytics-service     | Please manually install OpenSSL and try installing Prisma again.
intellidine-analytics-service     | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-analytics-service     | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-analytics-service     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-analytics-service     | 
intellidine-analytics-service     | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-analytics-service     | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-analytics-service     | 
intellidine-analytics-service     | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-analytics-service     |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-analytics-service     |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-analytics-service     |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-analytics-service     |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-analytics-service     |     at async Proxy.onModuleInit (/app/dist/prisma.service.js:19:9)
intellidine-analytics-service     |     at async Promise.all (index 0)
intellidine-analytics-service     |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-analytics-service     |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-analytics-service     |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-analytics-service     |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-analytics-service     |   clientVersion: '5.22.0',
intellidine-analytics-service     |   errorCode: undefined
intellidine-analytics-service     | }
intellidine-analytics-service     | 
intellidine-analytics-service     | Node.js v20.19.5
intellidine-zookeeper             | [2025-10-21 17:27:22,847] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
intellidine-zookeeper             | [2025-10-21 17:27:22,903] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
intellidine-zookeeper             | [2025-10-21 17:27:22,907] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
intellidine-zookeeper             | [2025-10-21 17:27:22,907] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
intellidine-zookeeper             | [2025-10-21 17:27:22,908] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
intellidine-zookeeper             | [2025-10-21 17:27:22,932] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
intellidine-zookeeper             | [2025-10-21 17:27:22,935] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
intellidine-zookeeper             | [2025-10-21 17:27:22,936] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
intellidine-zookeeper             | [2025-10-21 17:27:22,942] WARN Either no config or no quorum defined in config, running in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
intellidine-zookeeper             | [2025-10-21 17:27:22,950] INFO Log4j 1.2 jmx support not found; jmx disabled. (org.apache.zookeeper.jmx.ManagedUtil)
[Kintellidine-analytics-service exited with code 1 (restarting)
intellidine-zookeeper             | [2025-10-21 17:27:22,952] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
intellidine-zookeeper             | [2025-10-21 17:27:22,955] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
intellidine-zookeeper             | [2025-10-21 17:27:22,961] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
intellidine-zookeeper             | [2025-10-21 17:27:22,963] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
intellidine-zookeeper             | [2025-10-21 17:27:22,963] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
intellidine-zookeeper             | [2025-10-21 17:27:22,963] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
intellidine-zookeeper             | [2025-10-21 17:27:23,037] INFO ServerMetrics initialized with provider org.apache.zookeeper.metrics.impl.DefaultMetricsProvider@68e965f5 (org.apache.zookeeper.server.ServerMetrics)
intellidine-zookeeper             | [2025-10-21 17:27:23,052] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
intellidine-zookeeper             | [2025-10-21 17:27:23,163] INFO  (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,163] INFO   ______                  _                                           (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,164] INFO  |___  /                 | |                                          (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,164] INFO     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __    (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,164] INFO    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__| (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,164] INFO   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |     (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,164] INFO  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_| (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,164] INFO                                               | |                      (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,164] INFO                                               |_|                      (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,164] INFO  (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,169] INFO Server environment:zookeeper.version=3.6.4--d65253dcf68e9097c6e95a126463fd5fdeb4521c, built on 12/18/2022 18:10 GMT (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,170] INFO Server environment:host.name=2e5b1655ef6f (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,170] INFO Server environment:java.version=11.0.20 (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,170] INFO Server environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,170] INFO Server environment:java.home=/usr/lib/jvm/java-11-zulu-openjdk-ca (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,170] INFO Server environment:java.class.path=/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.13.5.jar:/usr/bin/../share/java/kafka/connect-runtime-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/kafka-raft-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.96.Final.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.39.1.jar:/usr/bin/../share/java/kafka/audience-annotations-0.13.0.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.2.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.4.jar:/usr/bin/../share/java/kafka/jersey-client-2.39.1.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/kafka-shell-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/jackson-databind-2.13.5.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.10.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/netty-transport-classes-epoll-4.1.96.Final.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/slf4j-reload4j-1.7.36.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jackson-core-2.13.5.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.5-1.jar:/usr/bin/../share/java/kafka/kafka-tools-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.13.5.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/kafka-storage-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/jersey-common-2.39.1.jar:/usr/bin/../share/java/kafka/kafka-group-coordinator-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/trogdor-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.96.Final.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.96.Final.jar:/usr/bin/../share/java/kafka/netty-common-4.1.96.Final.jar:/usr/bin/../share/java/kafka/connect-transforms-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/jline-3.22.0.jar:/usr/bin/../share/java/kafka/javax.activation-api-1.2.0.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/swagger-annotations-2.2.8.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.1.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.96.Final.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/reload4j-1.2.25.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.13.5.jar:/usr/bin/../share/java/kafka/connect-api-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.13.5.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.13.5.jar:/usr/bin/../share/java/kafka/kafka-streams-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/scala-library-2.13.10.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.39.1.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.10.0.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/kafka-clients-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jose4j-0.9.3.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/connect-mirror-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jersey-server-2.39.1.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.4.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/connect-json-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.10.0.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.36.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.96.Final.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.13.5.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/kafka-tools-api-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.3.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.2.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.4.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/plexus-utils-3.3.0.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.96.Final.jar:/usr/bin/../share/java/kafka/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.13.5.jar:/usr/bin/../share/java/kafka/javassist-3.29.2-GA.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.96.Final.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.4.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.39.1.jar:/usr/bin/../share/java/kafka/rocksdbjni-7.1.2.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,171] INFO Server environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,171] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,171] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,171] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,171] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,171] INFO Server environment:os.version=6.14.0-33-generic (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,171] INFO Server environment:user.name=appuser (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,171] INFO Server environment:user.home=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,174] INFO Server environment:user.dir=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,174] INFO Server environment:os.memory.free=491MB (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,175] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,175] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,175] INFO zookeeper.enableEagerACLCheck = false (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,175] INFO zookeeper.digest.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,175] INFO zookeeper.closeSessionTxn.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,175] INFO zookeeper.flushDelay=0 (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,176] INFO zookeeper.maxWriteQueuePollTime=0 (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,176] INFO zookeeper.maxBatchSize=1000 (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,178] INFO zookeeper.intBufferStartingSizeBytes = 1024 (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,183] INFO Weighed connection throttling is disabled (org.apache.zookeeper.server.BlueThrottle)
intellidine-zookeeper             | [2025-10-21 17:27:23,190] INFO minSessionTimeout set to 6000 (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,190] INFO maxSessionTimeout set to 60000 (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,194] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)
intellidine-zookeeper             | [2025-10-21 17:27:23,194] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)
intellidine-zookeeper             | [2025-10-21 17:27:23,198] INFO zookeeper.pathStats.slotCapacity = 60 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
intellidine-zookeeper             | [2025-10-21 17:27:23,199] INFO zookeeper.pathStats.slotDuration = 15 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
intellidine-zookeeper             | [2025-10-21 17:27:23,199] INFO zookeeper.pathStats.maxDepth = 6 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
intellidine-zookeeper             | [2025-10-21 17:27:23,200] INFO zookeeper.pathStats.initialDelay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
intellidine-zookeeper             | [2025-10-21 17:27:23,200] INFO zookeeper.pathStats.delay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
intellidine-zookeeper             | [2025-10-21 17:27:23,200] INFO zookeeper.pathStats.enabled = false (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
intellidine-zookeeper             | [2025-10-21 17:27:23,213] INFO The max bytes for all large requests are set to 104857600 (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,214] INFO The large request threshold is set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,215] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 clientPortListenBacklog -1 datadir /var/lib/zookeeper/log/version-2 snapdir /var/lib/zookeeper/data/version-2 (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:23,368] INFO Logging initialized @2276ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
intellidine-kafka                 | ===> Check if Zookeeper is healthy ...
intellidine-zookeeper             | [2025-10-21 17:27:24,335] WARN o.e.j.s.ServletContextHandler@5c7bfdc1{/,null,STOPPED} contextPath ends with /* (org.eclipse.jetty.server.handler.ContextHandler)
intellidine-zookeeper             | [2025-10-21 17:27:24,340] WARN Empty contextPath (org.eclipse.jetty.server.handler.ContextHandler)
intellidine-zookeeper             | [2025-10-21 17:27:24,442] INFO jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.20+8-LTS (org.eclipse.jetty.server.Server)
intellidine-nginx                 | /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
intellidine-nginx                 | /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
intellidine-nginx                 | /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
intellidine-nginx                 | 10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
intellidine-nginx                 | 10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
intellidine-nginx                 | /docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
intellidine-nginx                 | /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
intellidine-nginx                 | /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
intellidine-nginx                 | /docker-entrypoint.sh: Configuration complete; ready for start up
intellidine-zookeeper             | [2025-10-21 17:27:24,947] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
intellidine-zookeeper             | [2025-10-21 17:27:24,948] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
intellidine-zookeeper             | [2025-10-21 17:27:24,971] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session)
intellidine-zookeeper             | [2025-10-21 17:27:25,027] WARN ServletContext@o.e.j.s.ServletContextHandler@5c7bfdc1{/,null,STARTING} has uncovered http methods for path: /* (org.eclipse.jetty.security.SecurityHandler)
intellidine-zookeeper             | [2025-10-21 17:27:25,096] INFO Started o.e.j.s.ServletContextHandler@5c7bfdc1{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
intellidine-zookeeper             | [2025-10-21 17:27:25,169] INFO Started ServerConnector@5890e879{HTTP/1.1, (http/1.1)}{0.0.0.0:8080} (org.eclipse.jetty.server.AbstractConnector)
intellidine-zookeeper             | [2025-10-21 17:27:25,169] INFO Started @4078ms (org.eclipse.jetty.server.Server)
intellidine-zookeeper             | [2025-10-21 17:27:25,170] INFO Started AdminServer on address 0.0.0.0, port 8080 and command URL /commands (org.apache.zookeeper.server.admin.JettyAdminServer)
intellidine-zookeeper             | [2025-10-21 17:27:25,193] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
intellidine-zookeeper             | [2025-10-21 17:27:25,196] WARN maxCnxns is not configured, using default value 0. (org.apache.zookeeper.server.ServerCnxnFactory)
intellidine-zookeeper             | [2025-10-21 17:27:25,209] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 16 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)
intellidine-zookeeper             | [2025-10-21 17:27:25,221] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[Kintellidine-api-gateway exited with code 1 (restarting)
intellidine-zookeeper             | [2025-10-21 17:27:25,431] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)
intellidine-zookeeper             | [2025-10-21 17:27:25,434] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)
intellidine-zookeeper             | [2025-10-21 17:27:25,457] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
intellidine-zookeeper             | [2025-10-21 17:27:25,468] INFO zookeeper.commitLogCount=500 (org.apache.zookeeper.server.ZKDatabase)
intellidine-zookeeper             | [2025-10-21 17:27:25,590] INFO zookeeper.snapshot.compression.method = CHECKED (org.apache.zookeeper.server.persistence.SnapStream)
intellidine-zookeeper             | [2025-10-21 17:27:25,590] INFO Snapshotting: 0x0 to /var/lib/zookeeper/data/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
intellidine-zookeeper             | [2025-10-21 17:27:25,654] INFO Snapshot loaded in 182 ms, highest zxid is 0x0, digest is 1371985504 (org.apache.zookeeper.server.ZKDatabase)
intellidine-zookeeper             | [2025-10-21 17:27:25,655] INFO Snapshotting: 0x0 to /var/lib/zookeeper/data/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
intellidine-zookeeper             | [2025-10-21 17:27:25,656] INFO Snapshot taken in 1 ms (org.apache.zookeeper.server.ZooKeeperServer)
intellidine-zookeeper             | [2025-10-21 17:27:25,853] INFO zookeeper.request_throttler.shutdownTimeout = 10000 (org.apache.zookeeper.server.RequestThrottler)
intellidine-zookeeper             | [2025-10-21 17:27:25,850] INFO PrepRequestProcessor (sid:0) started, reconfigEnabled=false (org.apache.zookeeper.server.PrepRequestProcessor)
intellidine-zookeeper             | [2025-10-21 17:27:25,997] INFO Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0 (org.apache.zookeeper.server.ContainerManager)
intellidine-zookeeper             | [2025-10-21 17:27:26,002] INFO ZooKeeper audit is disabled. (org.apache.zookeeper.audit.ZKAuditProvider)
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:26 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:26 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +81ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:26 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/inventory}:[39m[38;5;3m +53ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:26 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/health, GET} route[39m[38;5;3m +12ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:26 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, POST} route[39m[38;5;3m +3ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:26 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, GET} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:26 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, GET} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:26 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, PATCH} route[39m[38;5;3m +7ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:26 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/deduct, PATCH} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:26 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/alerts, GET} route[39m[38;5;3m +27ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:26 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/stats, GET} route[39m[38;5;3m +12ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:26 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:26 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +70ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/payments}:[39m[38;5;3m +34ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/create-razorpay-order, POST} route[39m[38;5;3m +15ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/verify-razorpay, POST} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/confirm-cash, POST} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/health, GET} route[39m[38;5;3m +1ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/stats/daily, GET} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/:payment_id, GET} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments, GET} route[39m[38;5;3m +1ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +66ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +19ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +16ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, POST} route[39m[38;5;3m +3ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id, GET} route[39m[38;5;3m +2ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, GET} route[39m[38;5;3m +3ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/status, PATCH} route[39m[38;5;3m +5ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/cancel, PATCH} route[39m[38;5;3m +3ms[39m
intellidine-api-gateway           | node:internal/modules/cjs/loader:1210
intellidine-api-gateway           |   throw err;
intellidine-api-gateway           |   ^
intellidine-api-gateway           | 
intellidine-api-gateway           | Error: Cannot find module 'axios'
intellidine-api-gateway           | Require stack:
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/http.module.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/index.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/index.js
intellidine-api-gateway           | - /app/dist/src/app.module.js
intellidine-api-gateway           | - /app/dist/src/main.js
intellidine-api-gateway           |     at Module._resolveFilename (node:internal/modules/cjs/loader:1207:15)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1038:27)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19)
intellidine-api-gateway           |     at require (node:internal/modules/helpers:182:18)
intellidine-api-gateway           |     at Object.<anonymous> (/app/node_modules/@nestjs/axios/dist/http.module.js:16:33)
intellidine-api-gateway           |     at Module._compile (node:internal/modules/cjs/loader:1521:14)
intellidine-api-gateway           |     at Module._extensions..js (node:internal/modules/cjs/loader:1623:10)
intellidine-api-gateway           |     at Module.load (node:internal/modules/cjs/loader:1266:32)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1091:12)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19) {
intellidine-api-gateway           |   code: 'MODULE_NOT_FOUND',
intellidine-api-gateway           |   requireStack: [
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/http.module.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/index.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/index.js',
intellidine-api-gateway           |     '/app/dist/src/app.module.js',
intellidine-api-gateway           |     '/app/dist/src/main.js'
intellidine-api-gateway           |   ]
intellidine-api-gateway           | }
intellidine-api-gateway           | 
intellidine-api-gateway           | Node.js v20.19.5
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +73ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/analytics}:[39m[38;5;3m +23ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/health, GET} route[39m[38;5;3m +19ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/daily, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/recent, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/range, GET} route[39m[38;5;3m +3ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/aggregated, GET} route[39m[38;5;3m +2ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:27 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/orders/history, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:28.029Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.21.0.9:9092","broker":"kafka:9092","clientId":"analytics-service","stack":"Error: connect ECONNREFUSED 172.21.0.9:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1611:16)"}
intellidine-analytics-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:28.036Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.21.0.9:9092","retryCount":0,"retryTime":346}
intellidine-analytics-service     | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-analytics-service     | Please manually install OpenSSL and try installing Prisma again.
intellidine-analytics-service     | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-analytics-service     | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-analytics-service     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-analytics-service     | 
intellidine-analytics-service     | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-analytics-service     | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-analytics-service     | 
intellidine-analytics-service     | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-analytics-service     |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-analytics-service     |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-analytics-service     |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-analytics-service     |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-analytics-service     |     at async Proxy.onModuleInit (/app/dist/prisma.service.js:19:9)
intellidine-analytics-service     |     at async Promise.all (index 0)
intellidine-analytics-service     |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-analytics-service     |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-analytics-service     |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-analytics-service     |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-analytics-service     |   clientVersion: '5.22.0',
intellidine-analytics-service     |   errorCode: undefined
intellidine-analytics-service     | }
intellidine-analytics-service     | 
intellidine-analytics-service     | Node.js v20.19.5
[Kintellidine-inventory-service exited with code 1 (restarting)
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:28 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-kafka                 | [2025-10-21 17:27:28,594] INFO Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,595] INFO Client environment:host.name=d06fd8523869 (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,596] INFO Client environment:java.version=11.0.20 (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,596] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,596] INFO Client environment:java.home=/usr/lib/jvm/java-11-zulu-openjdk-ca (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,596] INFO Client environment:java.class.path=/usr/share/java/cp-base-new/kafka-raft-7.5.0-ccs.jar:/usr/share/java/cp-base-new/gson-2.9.0.jar:/usr/share/java/cp-base-new/scala-java8-compat_2.13-1.0.2.jar:/usr/share/java/cp-base-new/scala-logging_2.13-3.9.4.jar:/usr/share/java/cp-base-new/json-simple-1.1.1.jar:/usr/share/java/cp-base-new/jmx_prometheus_javaagent-0.18.0.jar:/usr/share/java/cp-base-new/jackson-core-2.14.2.jar:/usr/share/java/cp-base-new/kafka_2.13-7.5.0-ccs.jar:/usr/share/java/cp-base-new/zookeeper-3.6.3.jar:/usr/share/java/cp-base-new/scala-reflect-2.13.10.jar:/usr/share/java/cp-base-new/re2j-1.6.jar:/usr/share/java/cp-base-new/zookeeper-jute-3.6.3.jar:/usr/share/java/cp-base-new/slf4j-reload4j-1.7.36.jar:/usr/share/java/cp-base-new/kafka-server-common-7.5.0-ccs.jar:/usr/share/java/cp-base-new/zstd-jni-1.5.5-1.jar:/usr/share/java/cp-base-new/kafka-storage-7.5.0-ccs.jar:/usr/share/java/cp-base-new/common-utils-7.5.0.jar:/usr/share/java/cp-base-new/kafka-group-coordinator-7.5.0-ccs.jar:/usr/share/java/cp-base-new/utility-belt-7.5.0.jar:/usr/share/java/cp-base-new/argparse4j-0.7.0.jar:/usr/share/java/cp-base-new/audience-annotations-0.5.0.jar:/usr/share/java/cp-base-new/jackson-dataformat-yaml-2.14.2.jar:/usr/share/java/cp-base-new/reload4j-1.2.19.jar:/usr/share/java/cp-base-new/kafka-storage-api-7.5.0-ccs.jar:/usr/share/java/cp-base-new/commons-cli-1.4.jar:/usr/share/java/cp-base-new/metrics-core-2.2.0.jar:/usr/share/java/cp-base-new/scala-library-2.13.10.jar:/usr/share/java/cp-base-new/logredactor-1.0.12.jar:/usr/share/java/cp-base-new/jackson-dataformat-csv-2.14.2.jar:/usr/share/java/cp-base-new/jackson-databind-2.14.2.jar:/usr/share/java/cp-base-new/jackson-datatype-jdk8-2.14.2.jar:/usr/share/java/cp-base-new/kafka-clients-7.5.0-ccs.jar:/usr/share/java/cp-base-new/jackson-annotations-2.14.2.jar:/usr/share/java/cp-base-new/snakeyaml-2.0.jar:/usr/share/java/cp-base-new/jose4j-0.9.3.jar:/usr/share/java/cp-base-new/snappy-java-1.1.10.1.jar:/usr/share/java/cp-base-new/scala-collection-compat_2.13-2.10.0.jar:/usr/share/java/cp-base-new/slf4j-api-1.7.36.jar:/usr/share/java/cp-base-new/kafka-metadata-7.5.0-ccs.jar:/usr/share/java/cp-base-new/jackson-module-scala_2.13-2.14.2.jar:/usr/share/java/cp-base-new/jolokia-core-1.7.1.jar:/usr/share/java/cp-base-new/jopt-simple-5.0.4.jar:/usr/share/java/cp-base-new/disk-usage-agent-7.5.0.jar:/usr/share/java/cp-base-new/metrics-core-4.1.12.1.jar:/usr/share/java/cp-base-new/kafka-tools-api-7.5.0-ccs.jar:/usr/share/java/cp-base-new/jolokia-jvm-1.7.1.jar:/usr/share/java/cp-base-new/paranamer-2.8.jar:/usr/share/java/cp-base-new/logredactor-metrics-1.0.12.jar:/usr/share/java/cp-base-new/minimal-json-0.9.5.jar:/usr/share/java/cp-base-new/lz4-java-1.8.0.jar (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,597] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,597] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,597] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,597] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,597] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,597] INFO Client environment:os.version=6.14.0-33-generic (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,597] INFO Client environment:user.name=appuser (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,598] INFO Client environment:user.home=/home/appuser (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,598] INFO Client environment:user.dir=/home/appuser (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,598] INFO Client environment:os.memory.free=236MB (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,598] INFO Client environment:os.memory.max=3934MB (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,598] INFO Client environment:os.memory.total=246MB (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,620] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@221af3c0 (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:28,645] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
intellidine-kafka                 | [2025-10-21 17:27:28,675] INFO jute.maxbuffer value is 1048575 Bytes (org.apache.zookeeper.ClientCnxnSocket)
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:28 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +121ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:28 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +23ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:28 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +12ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:28 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/request-otp, POST} route[39m[38;5;3m +2ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:28 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/verify-otp, POST} route[39m[38;5;3m +2ms[39m
intellidine-kafka                 | [2025-10-21 17:27:28,730] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:28 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/staff/login, POST} route[39m[38;5;3m +2ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:28 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +8ms[39m
intellidine-kafka                 | [2025-10-21 17:27:28,784] INFO Opening socket connection to server zookeeper/172.21.0.5:2181. (org.apache.zookeeper.ClientCnxn)
intellidine-kafka                 | [2025-10-21 17:27:28,785] INFO SASL config status: Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:28 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-kafka                 | [2025-10-21 17:27:28,847] INFO Socket connection established, initiating session, client: /172.21.0.9:42136, server: zookeeper/172.21.0.5:2181 (org.apache.zookeeper.ClientCnxn)
intellidine-zookeeper             | [2025-10-21 17:27:28,906] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
intellidine-kafka                 | [2025-10-21 17:27:28,962] INFO Session establishment complete on server zookeeper/172.21.0.5:2181, session id = 0x10052d433540000, negotiated timeout = 40000 (org.apache.zookeeper.ClientCnxn)
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:28 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +184ms[39m
[Kintellidine-payment-service exited with code 1 (restarting)
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:28 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +18ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:29 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +12ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:29 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu, GET} route[39m[38;5;3m +2ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:29 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/categories, GET} route[39m[38;5;3m +2ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:29 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, GET} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:29 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items, POST} route[39m[38;5;3m +3ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:29 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, PATCH} route[39m[38;5;3m +2ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:29 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, DELETE} route[39m[38;5;3m +3ms[39m
intellidine-kafka                 | [2025-10-21 17:27:29,007] WARN An exception was thrown while closing send thread for session 0x10052d433540000. (org.apache.zookeeper.ClientCnxn)
intellidine-kafka                 | EndOfStreamException: Unable to read additional data from server sessionid 0x10052d433540000, likely server has closed socket
intellidine-kafka                 | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77)
intellidine-kafka                 | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
intellidine-kafka                 | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:29 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +9ms[39m
intellidine-kafka                 | [2025-10-21 17:27:29,123] INFO EventThread shut down for session: 0x10052d433540000 (org.apache.zookeeper.ClientCnxn)
intellidine-kafka                 | [2025-10-21 17:27:29,124] INFO Session: 0x10052d433540000 closed (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | Using log4j config /etc/kafka/log4j.properties
[Kintellidine-order-service exited with code 1 (restarting)
intellidine-kafka                 | ===> Launching ... 
intellidine-kafka                 | ===> Launching kafka ... 
[Kintellidine-api-gateway exited with code 1 (restarting)
[Kintellidine-analytics-service exited with code 1 (restarting)
[Kintellidine-auth-service exited with code 1 (restarting)
[Kintellidine-menu-service exited with code 1 (restarting)
intellidine-api-gateway           | node:internal/modules/cjs/loader:1210
intellidine-api-gateway           |   throw err;
intellidine-api-gateway           |   ^
intellidine-api-gateway           | 
intellidine-api-gateway           | Error: Cannot find module 'axios'
intellidine-api-gateway           | Require stack:
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/http.module.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/index.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/index.js
intellidine-api-gateway           | - /app/dist/src/app.module.js
intellidine-api-gateway           | - /app/dist/src/main.js
intellidine-api-gateway           |     at Module._resolveFilename (node:internal/modules/cjs/loader:1207:15)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1038:27)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19)
intellidine-api-gateway           |     at require (node:internal/modules/helpers:182:18)
intellidine-api-gateway           |     at Object.<anonymous> (/app/node_modules/@nestjs/axios/dist/http.module.js:16:33)
intellidine-api-gateway           |     at Module._compile (node:internal/modules/cjs/loader:1521:14)
intellidine-api-gateway           |     at Module._extensions..js (node:internal/modules/cjs/loader:1623:10)
intellidine-api-gateway           |     at Module.load (node:internal/modules/cjs/loader:1266:32)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1091:12)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19) {
intellidine-api-gateway           |   code: 'MODULE_NOT_FOUND',
intellidine-api-gateway           |   requireStack: [
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/http.module.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/index.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/index.js',
intellidine-api-gateway           |     '/app/dist/src/app.module.js',
intellidine-api-gateway           |     '/app/dist/src/main.js'
intellidine-api-gateway           |   ]
intellidine-api-gateway           | }
intellidine-api-gateway           | 
intellidine-api-gateway           | Node.js v20.19.5
[Kintellidine-api-gateway exited with code 1 (restarting)
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:31 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:31 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +69ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:31 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/inventory}:[39m[38;5;3m +17ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:31 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/health, GET} route[39m[38;5;3m +8ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:31 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, POST} route[39m[38;5;3m +3ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:31 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, GET} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:31 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, GET} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:31 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, PATCH} route[39m[38;5;3m +5ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:31 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/deduct, PATCH} route[39m[38;5;3m +6ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:31 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/alerts, GET} route[39m[38;5;3m +3ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:31 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/stats, GET} route[39m[38;5;3m +3ms[39m
intellidine-inventory-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:31.851Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.21.0.9:9092","broker":"kafka:9092","clientId":"inventory-service","stack":"Error: connect ECONNREFUSED 172.21.0.9:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1611:16)"}
intellidine-inventory-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:31.862Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.21.0.9:9092","retryCount":0,"retryTime":296}
intellidine-inventory-service     | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-inventory-service     | Please manually install OpenSSL and try installing Prisma again.
intellidine-inventory-service     | [31m[Nest] 1  - [39m10/21/2025, 5:27:31 PM [31m  ERROR[39m [38;5;3m[InventoryService] [39m[31mâ Failed to start Inventory Service:[39m
intellidine-inventory-service     | [31m[Nest] 1  - [39m10/21/2025, 5:27:31 PM [31m  ERROR[39m [38;5;3m[InventoryService] [39m[31mPrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-inventory-service     | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-inventory-service     | 
intellidine-inventory-service     | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +91ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/payments}:[39m[38;5;3m +34ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/create-razorpay-order, POST} route[39m[38;5;3m +14ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/verify-razorpay, POST} route[39m[38;5;3m +1ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/confirm-cash, POST} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/health, GET} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/stats/daily, GET} route[39m[38;5;3m +1ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/:payment_id, GET} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments, GET} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | {"level":"WARN","timestamp":"2025-10-21T17:27:32.326Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
intellidine-payment-service       | {"level":"ERROR","timestamp":"2025-10-21T17:27:32.401Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.21.0.9:9092","broker":"kafka:9092","clientId":"payment-service","stack":"Error: connect ECONNREFUSED 172.21.0.9:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1611:16)"}
intellidine-payment-service       | {"level":"ERROR","timestamp":"2025-10-21T17:27:32.411Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.21.0.9:9092","retryCount":0,"retryTime":295}
intellidine-payment-service       | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-payment-service       | Please manually install OpenSSL and try installing Prisma again.
intellidine-payment-service       | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-payment-service       | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-payment-service       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-payment-service       | 
intellidine-payment-service       | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-payment-service       | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-payment-service       | 
intellidine-payment-service       | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-payment-service       |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-payment-service       |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-payment-service       |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-payment-service       |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-payment-service       |     at async Proxy.onModuleInit (/app/dist/src/prisma.service.js:14:9)
intellidine-payment-service       |     at async Promise.all (index 1)
intellidine-payment-service       |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-payment-service       |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-payment-service       |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-payment-service       |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-payment-service       |   clientVersion: '5.22.0',
intellidine-payment-service       |   errorCode: undefined
intellidine-payment-service       | }
intellidine-payment-service       | 
intellidine-payment-service       | Node.js v20.19.5
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +67ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +23ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +18ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, POST} route[39m[38;5;3m +3ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id, GET} route[39m[38;5;3m +2ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, GET} route[39m[38;5;3m +4ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/status, PATCH} route[39m[38;5;3m +16ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:32 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/cancel, PATCH} route[39m[38;5;3m +5ms[39m
intellidine-order-service         | {"level":"WARN","timestamp":"2025-10-21T17:27:32.853Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
intellidine-order-service         | {"level":"ERROR","timestamp":"2025-10-21T17:27:32.922Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.21.0.9:9092","broker":"kafka:9092","clientId":"order-service","stack":"Error: connect ECONNREFUSED 172.21.0.9:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1611:16)"}
intellidine-order-service         | {"level":"ERROR","timestamp":"2025-10-21T17:27:32.927Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.21.0.9:9092","retryCount":0,"retryTime":97}
intellidine-order-service         | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-order-service         | Please manually install OpenSSL and try installing Prisma again.
intellidine-order-service         | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-order-service         | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-order-service         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-order-service         | 
intellidine-order-service         | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-order-service         | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-order-service         | 
intellidine-order-service         | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-order-service         |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-order-service         |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-order-service         |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-order-service         |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-order-service         |     at async Proxy.onModuleInit (/app/dist/src/prisma.service.js:14:9)
intellidine-order-service         |     at async Promise.all (index 1)
intellidine-order-service         |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-order-service         |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-order-service         |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-order-service         |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-order-service         |   clientVersion: '5.22.0',
intellidine-order-service         |   errorCode: undefined
intellidine-order-service         | }
intellidine-order-service         | 
intellidine-order-service         | Node.js v20.19.5
intellidine-kafka                 | [2025-10-21 17:27:33,106] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:33 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
[Kintellidine-inventory-service exited with code 1 (restarting)
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:33 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +76ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:33 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/analytics}:[39m[38;5;3m +29ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:33 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/health, GET} route[39m[38;5;3m +19ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:33 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/daily, GET} route[39m[38;5;3m +2ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:33 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/recent, GET} route[39m[38;5;3m +2ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:33 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/range, GET} route[39m[38;5;3m +2ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:33 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/aggregated, GET} route[39m[38;5;3m +3ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:33 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/orders/history, GET} route[39m[38;5;3m +2ms[39m
intellidine-analytics-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:33.562Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.21.0.9:9092","broker":"kafka:9092","clientId":"analytics-service","stack":"Error: connect ECONNREFUSED 172.21.0.9:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1611:16)"}
intellidine-analytics-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:33.576Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.21.0.9:9092","retryCount":0,"retryTime":311}
intellidine-analytics-service     | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-analytics-service     | Please manually install OpenSSL and try installing Prisma again.
intellidine-analytics-service     | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-analytics-service     | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-analytics-service     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-analytics-service     | 
intellidine-analytics-service     | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-analytics-service     | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-analytics-service     | 
intellidine-analytics-service     | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-analytics-service     |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-analytics-service     |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-analytics-service     |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-analytics-service     |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-analytics-service     |     at async Proxy.onModuleInit (/app/dist/prisma.service.js:19:9)
intellidine-analytics-service     |     at async Promise.all (index 0)
intellidine-analytics-service     |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-analytics-service     |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-analytics-service     |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-analytics-service     |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-analytics-service     |   clientVersion: '5.22.0',
intellidine-analytics-service     |   errorCode: undefined
intellidine-analytics-service     | }
intellidine-analytics-service     | 
intellidine-analytics-service     | Node.js v20.19.5
[Kintellidine-payment-service exited with code 1 (restarting)
intellidine-api-gateway           | node:internal/modules/cjs/loader:1210
intellidine-api-gateway           |   throw err;
intellidine-api-gateway           |   ^
intellidine-api-gateway           | 
intellidine-api-gateway           | Error: Cannot find module 'axios'
intellidine-api-gateway           | Require stack:
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/http.module.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/index.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/index.js
intellidine-api-gateway           | - /app/dist/src/app.module.js
intellidine-api-gateway           | - /app/dist/src/main.js
intellidine-api-gateway           |     at Module._resolveFilename (node:internal/modules/cjs/loader:1207:15)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1038:27)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19)
intellidine-api-gateway           |     at require (node:internal/modules/helpers:182:18)
intellidine-api-gateway           |     at Object.<anonymous> (/app/node_modules/@nestjs/axios/dist/http.module.js:16:33)
intellidine-api-gateway           |     at Module._compile (node:internal/modules/cjs/loader:1521:14)
intellidine-api-gateway           |     at Module._extensions..js (node:internal/modules/cjs/loader:1623:10)
intellidine-api-gateway           |     at Module.load (node:internal/modules/cjs/loader:1266:32)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1091:12)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19) {
intellidine-api-gateway           |   code: 'MODULE_NOT_FOUND',
intellidine-api-gateway           |   requireStack: [
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/http.module.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/index.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/index.js',
intellidine-api-gateway           |     '/app/dist/src/app.module.js',
intellidine-api-gateway           |     '/app/dist/src/main.js'
intellidine-api-gateway           |   ]
intellidine-api-gateway           | }
intellidine-api-gateway           | 
intellidine-api-gateway           | Node.js v20.19.5
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +108ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +27ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +10ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/request-otp, POST} route[39m[38;5;3m +2ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/verify-otp, POST} route[39m[38;5;3m +3ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/staff/login, POST} route[39m[38;5;3m +1ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +6ms[39m
intellidine-auth-service          | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-auth-service          | Please manually install OpenSSL and try installing Prisma again.
intellidine-auth-service          | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-auth-service          | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-auth-service          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-auth-service          | 
intellidine-auth-service          | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-auth-service          | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-auth-service          | 
intellidine-auth-service          | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-auth-service          |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-auth-service          |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-auth-service          |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599) {
intellidine-auth-service          |   clientVersion: '5.22.0',
intellidine-auth-service          |   errorCode: undefined
intellidine-auth-service          | }
intellidine-auth-service          | 
intellidine-auth-service          | Node.js v20.19.5
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
[Kintellidine-order-service exited with code 1 (restarting)
[Kintellidine-analytics-service exited with code 1 (restarting)
[Kintellidine-api-gateway exited with code 1 (restarting)
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +134ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +12ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +10ms[39m
intellidine-kafka                 | [2025-10-21 17:27:34,637] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu, GET} route[39m[38;5;3m +2ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/categories, GET} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, GET} route[39m[38;5;3m +2ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items, POST} route[39m[38;5;3m +3ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, PATCH} route[39m[38;5;3m +2ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, DELETE} route[39m[38;5;3m +2ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:34 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +9ms[39m
intellidine-menu-service          | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-menu-service          | Please manually install OpenSSL and try installing Prisma again.
intellidine-menu-service          | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-menu-service          | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-menu-service          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-menu-service          | 
intellidine-menu-service          | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-menu-service          | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-menu-service          | 
intellidine-menu-service          | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-menu-service          |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-menu-service          |     at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
intellidine-menu-service          |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-menu-service          |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599) {
intellidine-menu-service          |   clientVersion: '5.22.0',
intellidine-menu-service          |   errorCode: undefined
intellidine-menu-service          | }
intellidine-menu-service          | 
intellidine-menu-service          | Node.js v20.19.5
intellidine-kafka                 | [2025-10-21 17:27:34,969] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
intellidine-kafka                 | [2025-10-21 17:27:34,974] INFO starting (kafka.server.KafkaServer)
intellidine-kafka                 | [2025-10-21 17:27:34,975] INFO Connecting to zookeeper on zookeeper:2181 (kafka.server.KafkaServer)
intellidine-kafka                 | [2025-10-21 17:27:35,020] INFO [ZooKeeperClient Kafka server] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
intellidine-kafka                 | [2025-10-21 17:27:35,039] INFO Client environment:zookeeper.version=3.6.4--d65253dcf68e9097c6e95a126463fd5fdeb4521c, built on 12/18/2022 18:10 GMT (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,039] INFO Client environment:host.name=d06fd8523869 (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,040] INFO Client environment:java.version=11.0.20 (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,040] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,040] INFO Client environment:java.home=/usr/lib/jvm/java-11-zulu-openjdk-ca (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,040] INFO Client environment:java.class.path=/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.13.5.jar:/usr/bin/../share/java/kafka/connect-runtime-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/kafka-raft-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.96.Final.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.39.1.jar:/usr/bin/../share/java/kafka/audience-annotations-0.13.0.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.2.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.4.jar:/usr/bin/../share/java/kafka/jersey-client-2.39.1.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/kafka-shell-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/jackson-databind-2.13.5.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.10.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/netty-transport-classes-epoll-4.1.96.Final.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/slf4j-reload4j-1.7.36.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jackson-core-2.13.5.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.5-1.jar:/usr/bin/../share/java/kafka/kafka-tools-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.13.5.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/kafka-storage-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/jersey-common-2.39.1.jar:/usr/bin/../share/java/kafka/kafka-group-coordinator-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/trogdor-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.96.Final.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.96.Final.jar:/usr/bin/../share/java/kafka/netty-common-4.1.96.Final.jar:/usr/bin/../share/java/kafka/connect-transforms-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/jline-3.22.0.jar:/usr/bin/../share/java/kafka/javax.activation-api-1.2.0.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/swagger-annotations-2.2.8.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.1.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.96.Final.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/reload4j-1.2.25.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.13.5.jar:/usr/bin/../share/java/kafka/connect-api-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.13.5.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.13.5.jar:/usr/bin/../share/java/kafka/kafka-streams-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/scala-library-2.13.10.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.39.1.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.10.0.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/kafka-clients-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jose4j-0.9.3.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/connect-mirror-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jersey-server-2.39.1.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.4.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/connect-json-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.10.0.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.36.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.96.Final.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.13.5.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/kafka-tools-api-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.3.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.2.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.4.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/plexus-utils-3.3.0.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.96.Final.jar:/usr/bin/../share/java/kafka/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.13.5.jar:/usr/bin/../share/java/kafka/javassist-3.29.2-GA.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.51.v20230217.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.96.Final.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.5.0-ccs.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.4.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.39.1.jar:/usr/bin/../share/java/kafka/rocksdbjni-7.1.2.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,041] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,041] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,041] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,041] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,042] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,042] INFO Client environment:os.version=6.14.0-33-generic (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,042] INFO Client environment:user.name=appuser (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,043] INFO Client environment:user.home=/home/appuser (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,043] INFO Client environment:user.dir=/home/appuser (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,043] INFO Client environment:os.memory.free=1010MB (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,044] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,044] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,055] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@2d6764b2 (org.apache.zookeeper.ZooKeeper)
intellidine-kafka                 | [2025-10-21 17:27:35,071] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)
intellidine-kafka                 | [2025-10-21 17:27:35,089] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)
intellidine-kafka                 | [2025-10-21 17:27:35,103] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
intellidine-kafka                 | [2025-10-21 17:27:35,119] INFO Opening socket connection to server zookeeper/172.21.0.5:2181. (org.apache.zookeeper.ClientCnxn)
intellidine-kafka                 | [2025-10-21 17:27:35,137] INFO Socket connection established, initiating session, client: /172.21.0.9:32864, server: zookeeper/172.21.0.5:2181 (org.apache.zookeeper.ClientCnxn)
intellidine-kafka                 | [2025-10-21 17:27:35,179] INFO Session establishment complete on server zookeeper/172.21.0.5:2181, session id = 0x10052d433540001, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
intellidine-kafka                 | [2025-10-21 17:27:35,199] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[Kintellidine-auth-service exited with code 1 (restarting)
[Kintellidine-menu-service exited with code 1 (restarting)
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +64ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/inventory}:[39m[38;5;3m +14ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/health, GET} route[39m[38;5;3m +13ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, POST} route[39m[38;5;3m +4ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, GET} route[39m[38;5;3m +4ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, GET} route[39m[38;5;3m +4ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, PATCH} route[39m[38;5;3m +8ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/deduct, PATCH} route[39m[38;5;3m +5ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/alerts, GET} route[39m[38;5;3m +6ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/stats, GET} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:36.266Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.21.0.9:9092","broker":"kafka:9092","clientId":"inventory-service","stack":"Error: connect ECONNREFUSED 172.21.0.9:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1611:16)"}
intellidine-inventory-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:36.275Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.21.0.9:9092","retryCount":0,"retryTime":341}
intellidine-inventory-service     | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-inventory-service     | Please manually install OpenSSL and try installing Prisma again.
intellidine-inventory-service     | [31m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [31m  ERROR[39m [38;5;3m[InventoryService] [39m[31mâ Failed to start Inventory Service:[39m
intellidine-inventory-service     | [31m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [31m  ERROR[39m [38;5;3m[InventoryService] [39m[31mPrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-inventory-service     | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-inventory-service     | 
intellidine-inventory-service     | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)[39m
intellidine-kafka                 | [2025-10-21 17:27:36,353] INFO Cluster ID = MPO88b50QWCU7T8thv4YQg (kafka.server.KafkaServer)
intellidine-kafka                 | [2025-10-21 17:27:36,360] WARN No meta.properties file under dir /var/lib/kafka/data/meta.properties (kafka.server.BrokerMetadataCheckpoint)
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +70ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/payments}:[39m[38;5;3m +30ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/create-razorpay-order, POST} route[39m[38;5;3m +17ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/verify-razorpay, POST} route[39m[38;5;3m +1ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/confirm-cash, POST} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/health, GET} route[39m[38;5;3m +1ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/stats/daily, GET} route[39m[38;5;3m +3ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/:payment_id, GET} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:36 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments, GET} route[39m[38;5;3m +2ms[39m
intellidine-kafka                 | [2025-10-21 17:27:36,543] INFO KafkaConfig values: 
intellidine-kafka                 | 	advertised.listeners = PLAINTEXT://kafka:9092
intellidine-kafka                 | 	alter.config.policy.class.name = null
intellidine-kafka                 | 	alter.log.dirs.replication.quota.window.num = 11
intellidine-kafka                 | 	alter.log.dirs.replication.quota.window.size.seconds = 1
intellidine-kafka                 | 	authorizer.class.name = 
intellidine-kafka                 | 	auto.create.topics.enable = true
intellidine-kafka                 | 	auto.include.jmx.reporter = true
intellidine-kafka                 | 	auto.leader.rebalance.enable = true
intellidine-kafka                 | 	background.threads = 10
intellidine-kafka                 | 	broker.heartbeat.interval.ms = 2000
intellidine-kafka                 | 	broker.id = 1
intellidine-kafka                 | 	broker.id.generation.enable = true
intellidine-kafka                 | 	broker.rack = null
intellidine-kafka                 | 	broker.session.timeout.ms = 9000
intellidine-kafka                 | 	client.quota.callback.class = null
intellidine-kafka                 | 	compression.type = producer
intellidine-kafka                 | 	connection.failed.authentication.delay.ms = 100
intellidine-kafka                 | 	connections.max.idle.ms = 600000
intellidine-kafka                 | 	connections.max.reauth.ms = 0
intellidine-kafka                 | 	control.plane.listener.name = null
intellidine-kafka                 | 	controlled.shutdown.enable = true
intellidine-kafka                 | 	controlled.shutdown.max.retries = 3
intellidine-kafka                 | 	controlled.shutdown.retry.backoff.ms = 5000
intellidine-kafka                 | 	controller.listener.names = null
intellidine-kafka                 | 	controller.quorum.append.linger.ms = 25
intellidine-kafka                 | 	controller.quorum.election.backoff.max.ms = 1000
intellidine-kafka                 | 	controller.quorum.election.timeout.ms = 1000
intellidine-kafka                 | 	controller.quorum.fetch.timeout.ms = 2000
intellidine-kafka                 | 	controller.quorum.request.timeout.ms = 2000
intellidine-kafka                 | 	controller.quorum.retry.backoff.ms = 20
intellidine-kafka                 | 	controller.quorum.voters = []
intellidine-kafka                 | 	controller.quota.window.num = 11
intellidine-kafka                 | 	controller.quota.window.size.seconds = 1
intellidine-kafka                 | 	controller.socket.timeout.ms = 30000
intellidine-kafka                 | 	create.topic.policy.class.name = null
intellidine-kafka                 | 	default.replication.factor = 1
intellidine-kafka                 | 	delegation.token.expiry.check.interval.ms = 3600000
intellidine-kafka                 | 	delegation.token.expiry.time.ms = 86400000
intellidine-kafka                 | 	delegation.token.master.key = null
intellidine-kafka                 | 	delegation.token.max.lifetime.ms = 604800000
intellidine-kafka                 | 	delegation.token.secret.key = null
intellidine-kafka                 | 	delete.records.purgatory.purge.interval.requests = 1
intellidine-kafka                 | 	delete.topic.enable = true
intellidine-kafka                 | 	early.start.listeners = null
intellidine-kafka                 | 	fetch.max.bytes = 57671680
intellidine-kafka                 | 	fetch.purgatory.purge.interval.requests = 1000
intellidine-kafka                 | 	group.consumer.assignors = []
intellidine-kafka                 | 	group.consumer.heartbeat.interval.ms = 5000
intellidine-kafka                 | 	group.consumer.max.heartbeat.interval.ms = 15000
intellidine-kafka                 | 	group.consumer.max.session.timeout.ms = 60000
intellidine-kafka                 | 	group.consumer.max.size = 2147483647
intellidine-kafka                 | 	group.consumer.min.heartbeat.interval.ms = 5000
intellidine-kafka                 | 	group.consumer.min.session.timeout.ms = 45000
intellidine-kafka                 | 	group.consumer.session.timeout.ms = 45000
intellidine-kafka                 | 	group.coordinator.new.enable = false
intellidine-kafka                 | 	group.coordinator.threads = 1
intellidine-kafka                 | 	group.initial.rebalance.delay.ms = 3000
intellidine-kafka                 | 	group.max.session.timeout.ms = 1800000
intellidine-kafka                 | 	group.max.size = 2147483647
intellidine-kafka                 | 	group.min.session.timeout.ms = 6000
intellidine-kafka                 | 	initial.broker.registration.timeout.ms = 60000
intellidine-kafka                 | 	inter.broker.listener.name = null
intellidine-kafka                 | 	inter.broker.protocol.version = 3.5-IV2
intellidine-kafka                 | 	kafka.metrics.polling.interval.secs = 10
intellidine-kafka                 | 	kafka.metrics.reporters = []
intellidine-kafka                 | 	leader.imbalance.check.interval.seconds = 300
intellidine-kafka                 | 	leader.imbalance.per.broker.percentage = 10
intellidine-kafka                 | 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
intellidine-kafka                 | 	listeners = PLAINTEXT://0.0.0.0:9092
intellidine-kafka                 | 	log.cleaner.backoff.ms = 15000
intellidine-kafka                 | 	log.cleaner.dedupe.buffer.size = 134217728
intellidine-kafka                 | 	log.cleaner.delete.retention.ms = 86400000
intellidine-kafka                 | 	log.cleaner.enable = true
intellidine-kafka                 | 	log.cleaner.io.buffer.load.factor = 0.9
intellidine-kafka                 | 	log.cleaner.io.buffer.size = 524288
intellidine-kafka                 | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
intellidine-kafka                 | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
intellidine-kafka                 | 	log.cleaner.min.cleanable.ratio = 0.5
intellidine-kafka                 | 	log.cleaner.min.compaction.lag.ms = 0
intellidine-kafka                 | 	log.cleaner.threads = 1
intellidine-kafka                 | 	log.cleanup.policy = [delete]
intellidine-kafka                 | 	log.dir = /tmp/kafka-logs
intellidine-kafka                 | 	log.dirs = /var/lib/kafka/data
intellidine-kafka                 | 	log.flush.interval.messages = 9223372036854775807
intellidine-kafka                 | 	log.flush.interval.ms = null
intellidine-kafka                 | 	log.flush.offset.checkpoint.interval.ms = 60000
intellidine-kafka                 | 	log.flush.scheduler.interval.ms = 9223372036854775807
intellidine-kafka                 | 	log.flush.start.offset.checkpoint.interval.ms = 60000
intellidine-kafka                 | 	log.index.interval.bytes = 4096
intellidine-kafka                 | 	log.index.size.max.bytes = 10485760
intellidine-kafka                 | 	log.message.downconversion.enable = true
intellidine-kafka                 | 	log.message.format.version = 3.0-IV1
intellidine-kafka                 | 	log.message.timestamp.difference.max.ms = 9223372036854775807
intellidine-kafka                 | 	log.message.timestamp.type = CreateTime
intellidine-kafka                 | 	log.preallocate = false
intellidine-kafka                 | 	log.retention.bytes = -1
intellidine-kafka                 | 	log.retention.check.interval.ms = 300000
intellidine-kafka                 | 	log.retention.hours = 168
intellidine-kafka                 | 	log.retention.minutes = null
intellidine-kafka                 | 	log.retention.ms = null
intellidine-kafka                 | 	log.roll.hours = 168
intellidine-kafka                 | 	log.roll.jitter.hours = 0
intellidine-kafka                 | 	log.roll.jitter.ms = null
intellidine-kafka                 | 	log.roll.ms = null
intellidine-kafka                 | 	log.segment.bytes = 1073741824
intellidine-kafka                 | 	log.segment.delete.delay.ms = 60000
intellidine-kafka                 | 	max.connection.creation.rate = 2147483647
intellidine-kafka                 | 	max.connections = 2147483647
intellidine-kafka                 | 	max.connections.per.ip = 2147483647
intellidine-kafka                 | 	max.connections.per.ip.overrides = 
intellidine-kafka                 | 	max.incremental.fetch.session.cache.slots = 1000
intellidine-kafka                 | 	message.max.bytes = 1048588
intellidine-kafka                 | 	metadata.log.dir = null
intellidine-kafka                 | 	metadata.log.max.record.bytes.between.snapshots = 20971520
intellidine-kafka                 | 	metadata.log.max.snapshot.interval.ms = 3600000
intellidine-kafka                 | 	metadata.log.segment.bytes = 1073741824
intellidine-kafka                 | 	metadata.log.segment.min.bytes = 8388608
intellidine-kafka                 | 	metadata.log.segment.ms = 604800000
intellidine-kafka                 | 	metadata.max.idle.interval.ms = 500
intellidine-kafka                 | 	metadata.max.retention.bytes = 104857600
intellidine-kafka                 | 	metadata.max.retention.ms = 604800000
intellidine-kafka                 | 	metric.reporters = []
intellidine-kafka                 | 	metrics.num.samples = 2
intellidine-kafka                 | 	metrics.recording.level = INFO
intellidine-kafka                 | 	metrics.sample.window.ms = 30000
intellidine-kafka                 | 	min.insync.replicas = 1
intellidine-kafka                 | 	node.id = 1
intellidine-kafka                 | 	num.io.threads = 8
intellidine-kafka                 | 	num.network.threads = 3
intellidine-kafka                 | 	num.partitions = 1
intellidine-kafka                 | 	num.recovery.threads.per.data.dir = 1
intellidine-kafka                 | 	num.replica.alter.log.dirs.threads = null
intellidine-kafka                 | 	num.replica.fetchers = 1
intellidine-kafka                 | 	offset.metadata.max.bytes = 4096
intellidine-kafka                 | 	offsets.commit.required.acks = -1
intellidine-kafka                 | 	offsets.commit.timeout.ms = 5000
intellidine-kafka                 | 	offsets.load.buffer.size = 5242880
intellidine-kafka                 | 	offsets.retention.check.interval.ms = 600000
intellidine-kafka                 | 	offsets.retention.minutes = 10080
intellidine-kafka                 | 	offsets.topic.compression.codec = 0
intellidine-kafka                 | 	offsets.topic.num.partitions = 50
intellidine-kafka                 | 	offsets.topic.replication.factor = 1
intellidine-kafka                 | 	offsets.topic.segment.bytes = 104857600
intellidine-kafka                 | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
intellidine-kafka                 | 	password.encoder.iterations = 4096
intellidine-kafka                 | 	password.encoder.key.length = 128
intellidine-kafka                 | 	password.encoder.keyfactory.algorithm = null
intellidine-kafka                 | 	password.encoder.old.secret = null
intellidine-kafka                 | 	password.encoder.secret = null
intellidine-kafka                 | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
intellidine-kafka                 | 	process.roles = []
intellidine-kafka                 | 	producer.id.expiration.check.interval.ms = 600000
intellidine-kafka                 | 	producer.id.expiration.ms = 86400000
intellidine-kafka                 | 	producer.purgatory.purge.interval.requests = 1000
intellidine-kafka                 | 	queued.max.request.bytes = -1
intellidine-kafka                 | 	queued.max.requests = 500
intellidine-kafka                 | 	quota.window.num = 11
intellidine-kafka                 | 	quota.window.size.seconds = 1
intellidine-kafka                 | 	remote.log.index.file.cache.total.size.bytes = 1073741824
intellidine-kafka                 | 	remote.log.manager.task.interval.ms = 30000
intellidine-kafka                 | 	remote.log.manager.task.retry.backoff.max.ms = 30000
intellidine-kafka                 | 	remote.log.manager.task.retry.backoff.ms = 500
intellidine-kafka                 | 	remote.log.manager.task.retry.jitter = 0.2
intellidine-kafka                 | 	remote.log.manager.thread.pool.size = 10
intellidine-kafka                 | 	remote.log.metadata.manager.class.name = null
intellidine-kafka                 | 	remote.log.metadata.manager.class.path = null
intellidine-kafka                 | 	remote.log.metadata.manager.impl.prefix = null
intellidine-kafka                 | 	remote.log.metadata.manager.listener.name = null
intellidine-kafka                 | 	remote.log.reader.max.pending.tasks = 100
intellidine-kafka                 | 	remote.log.reader.threads = 10
intellidine-kafka                 | 	remote.log.storage.manager.class.name = null
intellidine-kafka                 | 	remote.log.storage.manager.class.path = null
intellidine-kafka                 | 	remote.log.storage.manager.impl.prefix = null
intellidine-kafka                 | 	remote.log.storage.system.enable = false
intellidine-kafka                 | 	replica.fetch.backoff.ms = 1000
intellidine-kafka                 | 	replica.fetch.max.bytes = 1048576
intellidine-kafka                 | 	replica.fetch.min.bytes = 1
intellidine-kafka                 | 	replica.fetch.response.max.bytes = 10485760
intellidine-kafka                 | 	replica.fetch.wait.max.ms = 500
intellidine-kafka                 | 	replica.high.watermark.checkpoint.interval.ms = 5000
intellidine-kafka                 | 	replica.lag.time.max.ms = 30000
intellidine-kafka                 | 	replica.selector.class = null
intellidine-kafka                 | 	replica.socket.receive.buffer.bytes = 65536
intellidine-kafka                 | 	replica.socket.timeout.ms = 30000
intellidine-kafka                 | 	replication.quota.window.num = 11
intellidine-kafka                 | 	replication.quota.window.size.seconds = 1
intellidine-kafka                 | 	request.timeout.ms = 30000
intellidine-kafka                 | 	reserved.broker.max.id = 1000
intellidine-kafka                 | 	sasl.client.callback.handler.class = null
intellidine-kafka                 | 	sasl.enabled.mechanisms = [GSSAPI]
intellidine-kafka                 | 	sasl.jaas.config = null
intellidine-kafka                 | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
intellidine-kafka                 | 	sasl.kerberos.min.time.before.relogin = 60000
intellidine-kafka                 | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
intellidine-kafka                 | 	sasl.kerberos.service.name = null
intellidine-kafka                 | 	sasl.kerberos.ticket.renew.jitter = 0.05
intellidine-kafka                 | 	sasl.kerberos.ticket.renew.window.factor = 0.8
intellidine-kafka                 | 	sasl.login.callback.handler.class = null
intellidine-kafka                 | 	sasl.login.class = null
intellidine-kafka                 | 	sasl.login.connect.timeout.ms = null
intellidine-kafka                 | 	sasl.login.read.timeout.ms = null
intellidine-kafka                 | 	sasl.login.refresh.buffer.seconds = 300
intellidine-kafka                 | 	sasl.login.refresh.min.period.seconds = 60
intellidine-kafka                 | 	sasl.login.refresh.window.factor = 0.8
intellidine-kafka                 | 	sasl.login.refresh.window.jitter = 0.05
intellidine-kafka                 | 	sasl.login.retry.backoff.max.ms = 10000
intellidine-kafka                 | 	sasl.login.retry.backoff.ms = 100
intellidine-kafka                 | 	sasl.mechanism.controller.protocol = GSSAPI
intellidine-kafka                 | 	sasl.mechanism.inter.broker.protocol = GSSAPI
intellidine-kafka                 | 	sasl.oauthbearer.clock.skew.seconds = 30
intellidine-kafka                 | 	sasl.oauthbearer.expected.audience = null
intellidine-kafka                 | 	sasl.oauthbearer.expected.issuer = null
intellidine-kafka                 | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
intellidine-kafka                 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
intellidine-kafka                 | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
intellidine-kafka                 | 	sasl.oauthbearer.jwks.endpoint.url = null
intellidine-kafka                 | 	sasl.oauthbearer.scope.claim.name = scope
intellidine-kafka                 | 	sasl.oauthbearer.sub.claim.name = sub
intellidine-kafka                 | 	sasl.oauthbearer.token.endpoint.url = null
intellidine-kafka                 | 	sasl.server.callback.handler.class = null
intellidine-kafka                 | 	sasl.server.max.receive.size = 524288
intellidine-kafka                 | 	security.inter.broker.protocol = PLAINTEXT
intellidine-kafka                 | 	security.providers = null
intellidine-kafka                 | 	server.max.startup.time.ms = 9223372036854775807
intellidine-kafka                 | 	socket.connection.setup.timeout.max.ms = 30000
intellidine-kafka                 | 	socket.connection.setup.timeout.ms = 10000
intellidine-kafka                 | 	socket.listen.backlog.size = 50
intellidine-kafka                 | 	socket.receive.buffer.bytes = 102400
intellidine-kafka                 | 	socket.request.max.bytes = 104857600
intellidine-kafka                 | 	socket.send.buffer.bytes = 102400
intellidine-kafka                 | 	ssl.cipher.suites = []
intellidine-kafka                 | 	ssl.client.auth = none
intellidine-kafka                 | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
intellidine-kafka                 | 	ssl.endpoint.identification.algorithm = https
intellidine-kafka                 | 	ssl.engine.factory.class = null
intellidine-kafka                 | 	ssl.key.password = null
intellidine-kafka                 | 	ssl.keymanager.algorithm = SunX509
intellidine-kafka                 | 	ssl.keystore.certificate.chain = null
intellidine-kafka                 | 	ssl.keystore.key = null
intellidine-kafka                 | 	ssl.keystore.location = null
intellidine-kafka                 | 	ssl.keystore.password = null
intellidine-kafka                 | 	ssl.keystore.type = JKS
intellidine-kafka                 | 	ssl.principal.mapping.rules = DEFAULT
intellidine-kafka                 | 	ssl.protocol = TLSv1.3
intellidine-kafka                 | 	ssl.provider = null
intellidine-kafka                 | 	ssl.secure.random.implementation = null
intellidine-kafka                 | 	ssl.trustmanager.algorithm = PKIX
intellidine-kafka                 | 	ssl.truststore.certificates = null
intellidine-kafka                 | 	ssl.truststore.location = null
intellidine-kafka                 | 	ssl.truststore.password = null
intellidine-kafka                 | 	ssl.truststore.type = JKS
intellidine-kafka                 | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
intellidine-kafka                 | 	transaction.max.timeout.ms = 900000
intellidine-kafka                 | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
intellidine-kafka                 | 	transaction.state.log.load.buffer.size = 5242880
intellidine-kafka                 | 	transaction.state.log.min.isr = 2
intellidine-kafka                 | 	transaction.state.log.num.partitions = 50
intellidine-kafka                 | 	transaction.state.log.replication.factor = 3
intellidine-kafka                 | 	transaction.state.log.segment.bytes = 104857600
intellidine-kafka                 | 	transactional.id.expiration.ms = 604800000
intellidine-kafka                 | 	unclean.leader.election.enable = false
intellidine-kafka                 | 	unstable.api.versions.enable = false
intellidine-kafka                 | 	zookeeper.clientCnxnSocket = null
intellidine-kafka                 | 	zookeeper.connect = zookeeper:2181
intellidine-kafka                 | 	zookeeper.connection.timeout.ms = null
intellidine-kafka                 | 	zookeeper.max.in.flight.requests = 10
intellidine-kafka                 | 	zookeeper.metadata.migration.enable = false
intellidine-kafka                 | 	zookeeper.session.timeout.ms = 18000
intellidine-kafka                 | 	zookeeper.set.acl = false
intellidine-kafka                 | 	zookeeper.ssl.cipher.suites = null
intellidine-kafka                 | 	zookeeper.ssl.client.enable = false
intellidine-kafka                 | 	zookeeper.ssl.crl.enable = false
intellidine-kafka                 | 	zookeeper.ssl.enabled.protocols = null
intellidine-kafka                 | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
intellidine-kafka                 | 	zookeeper.ssl.keystore.location = null
intellidine-kafka                 | 	zookeeper.ssl.keystore.password = null
intellidine-kafka                 | 	zookeeper.ssl.keystore.type = null
intellidine-kafka                 | 	zookeeper.ssl.ocsp.enable = false
intellidine-kafka                 | 	zookeeper.ssl.protocol = TLSv1.2
intellidine-kafka                 | 	zookeeper.ssl.truststore.location = null
intellidine-kafka                 | 	zookeeper.ssl.truststore.password = null
intellidine-kafka                 | 	zookeeper.ssl.truststore.type = null
intellidine-kafka                 |  (kafka.server.KafkaConfig)
intellidine-payment-service       | {"level":"WARN","timestamp":"2025-10-21T17:27:36.549Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
intellidine-payment-service       | {"level":"ERROR","timestamp":"2025-10-21T17:27:36.624Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.21.0.9:9092","broker":"kafka:9092","clientId":"payment-service","stack":"Error: connect ECONNREFUSED 172.21.0.9:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1611:16)"}
intellidine-payment-service       | {"level":"ERROR","timestamp":"2025-10-21T17:27:36.638Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.21.0.9:9092","retryCount":0,"retryTime":251}
[Kintellidine-inventory-service exited with code 1 (restarting)
intellidine-kafka                 | [2025-10-21 17:27:36,695] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
intellidine-kafka                 | [2025-10-21 17:27:36,700] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
intellidine-kafka                 | [2025-10-21 17:27:36,711] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
intellidine-kafka                 | [2025-10-21 17:27:36,718] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
intellidine-payment-service       | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-payment-service       | Please manually install OpenSSL and try installing Prisma again.
intellidine-payment-service       | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-payment-service       | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-payment-service       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-payment-service       | 
intellidine-payment-service       | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-payment-service       | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-payment-service       | 
intellidine-payment-service       | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-payment-service       |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-payment-service       |     at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
intellidine-payment-service       |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-payment-service       |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-payment-service       |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-payment-service       |     at async Proxy.onModuleInit (/app/dist/src/prisma.service.js:14:9)
intellidine-payment-service       |     at async Promise.all (index 1)
intellidine-payment-service       |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-payment-service       |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-payment-service       |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9) {
intellidine-payment-service       |   clientVersion: '5.22.0',
intellidine-payment-service       |   errorCode: undefined
intellidine-payment-service       | }
intellidine-payment-service       | 
intellidine-payment-service       | Node.js v20.19.5
intellidine-kafka                 | [2025-10-21 17:27:36,843] INFO Loading logs from log dirs ArraySeq(/var/lib/kafka/data) (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:36,866] INFO No logs found to be loaded in /var/lib/kafka/data (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:36,900] INFO Loaded 0 logs in 54ms (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:36,905] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:36,908] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:36,958] INFO Starting the log cleaner (kafka.log.LogCleaner)
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:37 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:37 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +100ms[39m
[Kintellidine-payment-service exited with code 1 (restarting)
intellidine-kafka                 | [2025-10-21 17:27:37,228] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:37 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +28ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:37 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +10ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:37 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, POST} route[39m[38;5;3m +4ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:37 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id, GET} route[39m[38;5;3m +2ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:37 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, GET} route[39m[38;5;3m +2ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:37 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/status, PATCH} route[39m[38;5;3m +9ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:37 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/cancel, PATCH} route[39m[38;5;3m +4ms[39m
intellidine-api-gateway           | node:internal/modules/cjs/loader:1210
intellidine-order-service         | {"level":"WARN","timestamp":"2025-10-21T17:27:37.292Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
intellidine-api-gateway           |   throw err;
intellidine-api-gateway           |   ^
intellidine-api-gateway           | 
intellidine-api-gateway           | Error: Cannot find module 'axios'
intellidine-api-gateway           | Require stack:
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/http.module.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/index.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/index.js
intellidine-api-gateway           | - /app/dist/src/app.module.js
intellidine-api-gateway           | - /app/dist/src/main.js
intellidine-api-gateway           |     at Module._resolveFilename (node:internal/modules/cjs/loader:1207:15)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1038:27)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19)
intellidine-api-gateway           |     at require (node:internal/modules/helpers:182:18)
intellidine-api-gateway           |     at Object.<anonymous> (/app/node_modules/@nestjs/axios/dist/http.module.js:16:33)
intellidine-api-gateway           |     at Module._compile (node:internal/modules/cjs/loader:1521:14)
intellidine-api-gateway           |     at Module._extensions..js (node:internal/modules/cjs/loader:1623:10)
intellidine-api-gateway           |     at Module.load (node:internal/modules/cjs/loader:1266:32)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1091:12)
intellidine-kafka                 | [2025-10-21 17:27:37,317] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19) {
intellidine-api-gateway           |   code: 'MODULE_NOT_FOUND',
intellidine-api-gateway           |   requireStack: [
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/http.module.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/index.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/index.js',
intellidine-api-gateway           |     '/app/dist/src/app.module.js',
intellidine-api-gateway           |     '/app/dist/src/main.js'
intellidine-api-gateway           |   ]
intellidine-api-gateway           | }
intellidine-api-gateway           | 
intellidine-api-gateway           | Node.js v20.19.5
intellidine-order-service         | {"level":"ERROR","timestamp":"2025-10-21T17:27:37.372Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.21.0.9:9092","broker":"kafka:9092","clientId":"order-service","stack":"Error: connect ECONNREFUSED 172.21.0.9:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1611:16)"}
intellidine-kafka                 | [2025-10-21 17:27:37,375] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)
intellidine-order-service         | {"level":"ERROR","timestamp":"2025-10-21T17:27:37.377Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.21.0.9:9092","retryCount":0,"retryTime":89}
intellidine-order-service         | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-order-service         | Please manually install OpenSSL and try installing Prisma again.
intellidine-order-service         | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-order-service         | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-order-service         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-order-service         | 
intellidine-order-service         | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-order-service         | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-order-service         | 
intellidine-order-service         | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-order-service         |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-order-service         |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-kafka                 | [2025-10-21 17:27:37,477] INFO [zk-broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.BrokerToControllerRequestThread)
intellidine-order-service         |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-order-service         |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-order-service         |     at async Proxy.onModuleInit (/app/dist/src/prisma.service.js:14:9)
intellidine-order-service         |     at async Promise.all (index 1)
intellidine-order-service         |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-order-service         |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-order-service         |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-order-service         |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-order-service         |   clientVersion: '5.22.0',
intellidine-order-service         |   errorCode: undefined
intellidine-order-service         | }
intellidine-order-service         | 
intellidine-order-service         | Node.js v20.19.5
[Kintellidine-api-gateway exited with code 1 (restarting)
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:38 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:38 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +109ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:38 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +24ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:38 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +9ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:38 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/request-otp, POST} route[39m[38;5;3m +2ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:38 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/verify-otp, POST} route[39m[38;5;3m +1ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:38 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/staff/login, POST} route[39m[38;5;3m +1ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:38 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +9ms[39m
[Kintellidine-order-service exited with code 1 (restarting)
intellidine-auth-service          | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-auth-service          | Please manually install OpenSSL and try installing Prisma again.
intellidine-auth-service          | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-auth-service          | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-auth-service          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-auth-service          | 
intellidine-auth-service          | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-auth-service          | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-auth-service          | 
intellidine-auth-service          | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-auth-service          |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-auth-service          |     at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
intellidine-auth-service          |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:39 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-auth-service          |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599) {
intellidine-auth-service          |   clientVersion: '5.22.0',
intellidine-auth-service          |   errorCode: undefined
intellidine-auth-service          | }
intellidine-auth-service          | 
intellidine-auth-service          | Node.js v20.19.5
intellidine-kafka                 | [2025-10-21 17:27:39,125] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:39 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +184ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:39 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +21ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:39 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +14ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:39 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu, GET} route[39m[38;5;3m +3ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:39 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/categories, GET} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:39 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, GET} route[39m[38;5;3m +2ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:39 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items, POST} route[39m[38;5;3m +3ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:39 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, PATCH} route[39m[38;5;3m +2ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:39 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, DELETE} route[39m[38;5;3m +4ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:39 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +10ms[39m
intellidine-kafka                 | [2025-10-21 17:27:39,245] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
intellidine-kafka                 | [2025-10-21 17:27:39,274] INFO [zk-broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.BrokerToControllerRequestThread)
intellidine-kafka                 | [2025-10-21 17:27:39,364] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
intellidine-menu-service          | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-menu-service          | Please manually install OpenSSL and try installing Prisma again.
intellidine-kafka                 | [2025-10-21 17:27:39,371] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
intellidine-menu-service          | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-kafka                 | [2025-10-21 17:27:39,410] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
intellidine-menu-service          | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-menu-service          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-menu-service          | 
intellidine-menu-service          | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-menu-service          | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-menu-service          | 
intellidine-menu-service          | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-menu-service          |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-menu-service          |     at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
intellidine-menu-service          |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-menu-service          |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599) {
intellidine-menu-service          |   clientVersion: '5.22.0',
intellidine-menu-service          |   errorCode: undefined
intellidine-menu-service          | }
intellidine-menu-service          | 
intellidine-menu-service          | Node.js v20.19.5
intellidine-kafka                 | [2025-10-21 17:27:39,431] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[Kintellidine-auth-service exited with code 1 (restarting)
intellidine-kafka                 | [2025-10-21 17:27:39,476] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
intellidine-kafka                 | [2025-10-21 17:27:39,601] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)
intellidine-kafka                 | [2025-10-21 17:27:39,703] INFO Stat of the created znode at /brokers/ids/1 is: 27,27,1761067659673,1761067659673,1,0,0,72148665385615361,194,0,27
intellidine-kafka                 |  (kafka.zk.KafkaZkClient)
intellidine-kafka                 | [2025-10-21 17:27:39,705] INFO Registered broker 1 at path /brokers/ids/1 with addresses: PLAINTEXT://kafka:9092, czxid (broker epoch): 27 (kafka.zk.KafkaZkClient)
intellidine-kafka                 | [2025-10-21 17:27:40,057] INFO [ControllerEventThread controllerId=1] Starting (kafka.controller.ControllerEventManager$ControllerEventThread)
intellidine-kafka                 | [2025-10-21 17:27:40,094] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
intellidine-kafka                 | [2025-10-21 17:27:40,179] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
intellidine-kafka                 | [2025-10-21 17:27:40,179] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-kafka                 | [2025-10-21 17:27:40,250] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +86ms[39m
intellidine-kafka                 | [2025-10-21 17:27:40,331] INFO [Controller id=1] 1 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:40,333] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:40,342] INFO [Controller id=1] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map()) (kafka.controller.KafkaController)
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/analytics}:[39m[38;5;3m +56ms[39m
intellidine-kafka                 | [2025-10-21 17:27:40,355] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/health, GET} route[39m[38;5;3m +23ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/daily, GET} route[39m[38;5;3m +2ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/recent, GET} route[39m[38;5;3m +3ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/range, GET} route[39m[38;5;3m +11ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/aggregated, GET} route[39m[38;5;3m +2ms[39m
intellidine-kafka                 | [2025-10-21 17:27:40,384] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/orders/history, GET} route[39m[38;5;3m +16ms[39m
intellidine-kafka                 | [2025-10-21 17:27:40,443] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:40,469] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:40,472] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-analytics-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:40.520Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.21.0.9:9092","broker":"kafka:9092","clientId":"analytics-service","stack":"Error: connect ECONNREFUSED 172.21.0.9:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1611:16)"}
intellidine-analytics-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:40.532Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.21.0.9:9092","retryCount":0,"retryTime":306}
intellidine-kafka                 | [2025-10-21 17:27:40,572] INFO [MetadataCache brokerId=1] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0). (kafka.server.metadata.ZkMetadataCache)
intellidine-kafka                 | [2025-10-21 17:27:40,575] INFO [Controller id=1] Registering handlers (kafka.controller.KafkaController)
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +76ms[39m
intellidine-analytics-service     | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-analytics-service     | Please manually install OpenSSL and try installing Prisma again.
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/inventory}:[39m[38;5;3m +26ms[39m
intellidine-analytics-service     | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-kafka                 | [2025-10-21 17:27:40,628] INFO [Controller id=1] Deleting log dir event notifications (kafka.controller.KafkaController)
intellidine-analytics-service     | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-analytics-service     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-analytics-service     | 
intellidine-analytics-service     | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-analytics-service     | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-analytics-service     | 
intellidine-analytics-service     | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-analytics-service     |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-analytics-service     |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-analytics-service     |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-analytics-service     |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-analytics-service     |     at async Proxy.onModuleInit (/app/dist/prisma.service.js:19:9)
intellidine-analytics-service     |     at async Promise.all (index 0)
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/health, GET} route[39m[38;5;3m +14ms[39m
intellidine-analytics-service     |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-analytics-service     |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-analytics-service     |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-analytics-service     |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-analytics-service     |   clientVersion: '5.22.0',
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, POST} route[39m[38;5;3m +3ms[39m
intellidine-analytics-service     |   errorCode: undefined
intellidine-analytics-service     | }
intellidine-analytics-service     | 
intellidine-analytics-service     | Node.js v20.19.5
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, GET} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, GET} route[39m[38;5;3m +3ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, PATCH} route[39m[38;5;3m +3ms[39m
intellidine-kafka                 | [2025-10-21 17:27:40,650] INFO [Controller id=1] Deleting isr change notifications (kafka.controller.KafkaController)
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/deduct, PATCH} route[39m[38;5;3m +3ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/alerts, GET} route[39m[38;5;3m +3ms[39m
intellidine-kafka                 | [2025-10-21 17:27:40,652] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/stats, GET} route[39m[38;5;3m +1ms[39m
intellidine-kafka                 | [2025-10-21 17:27:40,661] INFO [Controller id=1] Initializing controller context (kafka.controller.KafkaController)
intellidine-inventory-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:40.755Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.21.0.9:9092","broker":"kafka:9092","clientId":"inventory-service","stack":"Error: connect ECONNREFUSED 172.21.0.9:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1611:16)"}
intellidine-inventory-service     | {"level":"ERROR","timestamp":"2025-10-21T17:27:40.763Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.21.0.9:9092","retryCount":0,"retryTime":355}
intellidine-kafka                 | [2025-10-21 17:27:40,787] INFO [Controller id=1] Initialized broker epochs cache: HashMap(1 -> 27) (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:40,796] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
intellidine-kafka                 | [2025-10-21 17:27:40,804] DEBUG [Controller id=1] Register BrokerModifications handler for Set(1) (kafka.controller.KafkaController)
intellidine-inventory-service     | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-inventory-service     | Please manually install OpenSSL and try installing Prisma again.
intellidine-inventory-service     | [31m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [31m  ERROR[39m [38;5;3m[InventoryService] [39m[31mâ Failed to start Inventory Service:[39m
intellidine-inventory-service     | [31m[Nest] 1  - [39m10/21/2025, 5:27:40 PM [31m  ERROR[39m [38;5;3m[InventoryService] [39m[31mPrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-inventory-service     | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-inventory-service     | 
intellidine-inventory-service     | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)[39m
intellidine-kafka                 | [2025-10-21 17:27:40,847] DEBUG [Channel manager on controller 1]: Controller 1 trying to connect to broker 1 (kafka.controller.ControllerChannelManager)
intellidine-kafka                 | [2025-10-21 17:27:40,877] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
intellidine-kafka                 | [2025-10-21 17:27:40,910] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
[Kintellidine-menu-service exited with code 1 (restarting)
intellidine-kafka                 | [2025-10-21 17:27:40,931] INFO [RequestSendThread controllerId=1] Starting (kafka.controller.RequestSendThread)
intellidine-kafka                 | [2025-10-21 17:27:40,944] INFO [Controller id=1] Currently active brokers in the cluster: Set(1) (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:40,945] INFO [Controller id=1] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:40,946] INFO [Controller id=1] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:40,951] INFO [Controller id=1] Fetching topic deletions in progress (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:40,964] INFO [Controller id=1] List of topics to be deleted:  (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:40,969] INFO Kafka version: 7.5.0-ccs (org.apache.kafka.common.utils.AppInfoParser)
intellidine-kafka                 | [2025-10-21 17:27:40,970] INFO Kafka commitId: ff3c201baa948d97889dc26c99d7cdc23d038f2e (org.apache.kafka.common.utils.AppInfoParser)
intellidine-kafka                 | [2025-10-21 17:27:40,970] INFO Kafka startTimeMs: 1761067660935 (org.apache.kafka.common.utils.AppInfoParser)
intellidine-kafka                 | [2025-10-21 17:27:40,969] INFO [Controller id=1] List of topics ineligible for deletion:  (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:40,972] INFO [Controller id=1] Initializing topic deletion manager (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:40,973] INFO [Topic Deletion Manager 1] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager)
intellidine-kafka                 | [2025-10-21 17:27:40,982] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
intellidine-kafka                 | [2025-10-21 17:27:40,996] INFO [Controller id=1] Sending update metadata request (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:41,035] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 0 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:41,171] INFO [ReplicaStateMachine controllerId=1] Initializing replica state (kafka.controller.ZkReplicaStateMachine)
intellidine-kafka                 | [2025-10-21 17:27:41,179] INFO [ReplicaStateMachine controllerId=1] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine)
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:41 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-kafka                 | [2025-10-21 17:27:41,212] INFO [RequestSendThread controllerId=1] Controller 1 connected to kafka:9092 (id: 1 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
intellidine-kafka                 | [2025-10-21 17:27:41,225] INFO [ReplicaStateMachine controllerId=1] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine)
intellidine-kafka                 | [2025-10-21 17:27:41,231] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> HashMap() (kafka.controller.ZkReplicaStateMachine)
intellidine-kafka                 | [2025-10-21 17:27:41,233] INFO [PartitionStateMachine controllerId=1] Initializing partition state (kafka.controller.ZkPartitionStateMachine)
intellidine-kafka                 | [2025-10-21 17:27:41,238] INFO [PartitionStateMachine controllerId=1] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine)
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:41 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +60ms[39m
intellidine-kafka                 | [2025-10-21 17:27:41,250] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> HashMap() (kafka.controller.ZkPartitionStateMachine)
intellidine-kafka                 | [2025-10-21 17:27:41,251] INFO [Controller id=1] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController)
[Kintellidine-analytics-service exited with code 1 (restarting)
intellidine-kafka                 | [2025-10-21 17:27:41,268] INFO [Controller id=1] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:41 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/payments}:[39m[38;5;3m +39ms[39m
intellidine-kafka                 | [2025-10-21 17:27:41,284] INFO [Controller id=1] Partitions that completed preferred replica election:  (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:41,288] INFO [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:41,290] INFO [Controller id=1] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:41,297] INFO [Controller id=1] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController)
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:41 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/create-razorpay-order, POST} route[39m[38;5;3m +24ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:41 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/verify-razorpay, POST} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:41 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/confirm-cash, POST} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:41 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/health, GET} route[39m[38;5;3m +1ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:41 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/stats/daily, GET} route[39m[38;5;3m +3ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:41 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/:payment_id, GET} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:41 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments, GET} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | {"level":"WARN","timestamp":"2025-10-21T17:27:41.323Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
intellidine-api-gateway           | node:internal/modules/cjs/loader:1210
intellidine-api-gateway           |   throw err;
intellidine-api-gateway           |   ^
intellidine-api-gateway           | 
intellidine-api-gateway           | Error: Cannot find module 'axios'
intellidine-api-gateway           | Require stack:
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/http.module.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/index.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/index.js
intellidine-api-gateway           | - /app/dist/src/app.module.js
intellidine-api-gateway           | - /app/dist/src/main.js
intellidine-api-gateway           |     at Module._resolveFilename (node:internal/modules/cjs/loader:1207:15)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1038:27)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19)
intellidine-api-gateway           |     at require (node:internal/modules/helpers:182:18)
intellidine-api-gateway           |     at Object.<anonymous> (/app/node_modules/@nestjs/axios/dist/http.module.js:16:33)
intellidine-api-gateway           |     at Module._compile (node:internal/modules/cjs/loader:1521:14)
intellidine-api-gateway           |     at Module._extensions..js (node:internal/modules/cjs/loader:1623:10)
intellidine-api-gateway           |     at Module.load (node:internal/modules/cjs/loader:1266:32)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1091:12)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19) {
intellidine-api-gateway           |   code: 'MODULE_NOT_FOUND',
intellidine-api-gateway           |   requireStack: [
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/http.module.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/index.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/index.js',
intellidine-api-gateway           |     '/app/dist/src/app.module.js',
intellidine-api-gateway           |     '/app/dist/src/main.js'
intellidine-api-gateway           |   ]
intellidine-api-gateway           | }
intellidine-api-gateway           | 
intellidine-api-gateway           | Node.js v20.19.5
[Kintellidine-inventory-service exited with code 1 (restarting)
intellidine-kafka                 | [2025-10-21 17:27:41,438] INFO [Controller id=1] Starting the controller scheduler (kafka.controller.KafkaController)
intellidine-payment-service       | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-payment-service       | Please manually install OpenSSL and try installing Prisma again.
intellidine-payment-service       | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-payment-service       | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-payment-service       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-payment-service       | 
intellidine-payment-service       | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-payment-service       | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-payment-service       | 
intellidine-payment-service       | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-payment-service       |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-payment-service       |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-payment-service       |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-payment-service       |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-payment-service       |     at async Proxy.onModuleInit (/app/dist/src/prisma.service.js:14:9)
intellidine-payment-service       |     at async Promise.all (index 1)
intellidine-payment-service       |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-payment-service       |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-payment-service       |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-payment-service       |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-payment-service       |   clientVersion: '5.22.0',
intellidine-payment-service       |   errorCode: undefined
intellidine-payment-service       | }
intellidine-payment-service       | 
intellidine-payment-service       | Node.js v20.19.5
intellidine-kafka                 | [2025-10-21 17:27:41,612] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 0 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:41,626] INFO [zk-broker-1-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node kafka:9092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
intellidine-notification-service  | [32m[Nest] 1  - [39m10/21/2025, 5:27:41 PM [32m    LOG[39m [38;5;3m[KafkaConsumerService] [39m[32mKafka consumer connected[39m
intellidine-kafka                 | [2025-10-21 17:27:41,684] INFO [zk-broker-1-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node kafka:9092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
[Kintellidine-api-gateway exited with code 1 (restarting)
[Kintellidine-payment-service exited with code 1 (restarting)
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:41 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:42 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +53ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:42 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +16ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:42 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +9ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:42 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, POST} route[39m[38;5;3m +3ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:42 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id, GET} route[39m[38;5;3m +2ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:42 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, GET} route[39m[38;5;3m +2ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:42 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/status, PATCH} route[39m[38;5;3m +6ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:42 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/cancel, PATCH} route[39m[38;5;3m +2ms[39m
intellidine-order-service         | {"level":"WARN","timestamp":"2025-10-21T17:27:42.069Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:42 PM [32m    LOG[39m [38;5;3m[KafkaProducerService] [39m[32mKafka producer connected[39m
intellidine-order-service         | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-order-service         | Please manually install OpenSSL and try installing Prisma again.
intellidine-order-service         | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-order-service         | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-order-service         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-order-service         | 
intellidine-order-service         | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-order-service         | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-order-service         | 
intellidine-order-service         | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-order-service         |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-order-service         |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-order-service         |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-order-service         |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-order-service         |     at async Proxy.onModuleInit (/app/dist/src/prisma.service.js:14:9)
intellidine-order-service         |     at async Promise.all (index 1)
intellidine-order-service         |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-order-service         |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-order-service         |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-order-service         |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-order-service         |   clientVersion: '5.22.0',
intellidine-order-service         |   errorCode: undefined
intellidine-order-service         | }
intellidine-order-service         | 
intellidine-order-service         | Node.js v20.19.5
[Kintellidine-order-service exited with code 1 (restarting)
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +70ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +15ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +10ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/request-otp, POST} route[39m[38;5;3m +2ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/verify-otp, POST} route[39m[38;5;3m +1ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/staff/login, POST} route[39m[38;5;3m +1ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +7ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-auth-service          | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-auth-service          | Please manually install OpenSSL and try installing Prisma again.
intellidine-auth-service          | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-auth-service          | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-auth-service          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-auth-service          | 
intellidine-auth-service          | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-auth-service          | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-auth-service          | 
intellidine-auth-service          | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-auth-service          |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-auth-service          |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-auth-service          |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599) {
intellidine-auth-service          |   clientVersion: '5.22.0',
intellidine-auth-service          |   errorCode: undefined
intellidine-auth-service          | }
intellidine-auth-service          | 
intellidine-auth-service          | Node.js v20.19.5
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +123ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +13ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +11ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu, GET} route[39m[38;5;3m +2ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/categories, GET} route[39m[38;5;3m +2ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, GET} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items, POST} route[39m[38;5;3m +4ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, PATCH} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, DELETE} route[39m[38;5;3m +2ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:43 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +8ms[39m
intellidine-menu-service          | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-menu-service          | Please manually install OpenSSL and try installing Prisma again.
intellidine-menu-service          | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-menu-service          | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-menu-service          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-menu-service          | 
intellidine-menu-service          | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-menu-service          | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-menu-service          | 
intellidine-menu-service          | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-menu-service          |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-menu-service          |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-menu-service          |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599) {
intellidine-menu-service          |   clientVersion: '5.22.0',
intellidine-menu-service          |   errorCode: undefined
intellidine-menu-service          | }
intellidine-menu-service          | 
intellidine-menu-service          | Node.js v20.19.5
[Kintellidine-auth-service exited with code 1 (restarting)
[Kintellidine-menu-service exited with code 1 (restarting)
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +54ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/inventory}:[39m[38;5;3m +13ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/health, GET} route[39m[38;5;3m +5ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, POST} route[39m[38;5;3m +3ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, GET} route[39m[38;5;3m +1ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, GET} route[39m[38;5;3m +1ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, PATCH} route[39m[38;5;3m +4ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/deduct, PATCH} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/alerts, GET} route[39m[38;5;3m +3ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/stats, GET} route[39m[38;5;3m +1ms[39m
intellidine-inventory-service     | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-inventory-service     | Please manually install OpenSSL and try installing Prisma again.
intellidine-inventory-service     | [31m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [31m  ERROR[39m [38;5;3m[InventoryService] [39m[31mâ Failed to start Inventory Service:[39m
intellidine-inventory-service     | [31m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [31m  ERROR[39m [38;5;3m[InventoryService] [39m[31mPrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-inventory-service     | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-inventory-service     | 
intellidine-inventory-service     | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)[39m
intellidine-kafka                 | [2025-10-21 17:27:44,459] INFO Creating topic order.completed with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
intellidine-kafka                 | [2025-10-21 17:27:44,530] INFO [Controller id=1] New topics: [Set(order.completed)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(order.completed,Some(5mIbd_MQRrmhcbEnJzBgnw),Map(order.completed-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:44,532] INFO [Controller id=1] New partition creation callback for order.completed-0 (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:44,539] INFO [Controller id=1 epoch=1] Changed partition order.completed-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:44,540] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:44,550] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition order.completed-0 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:44,551] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:44,642] INFO [Controller id=1 epoch=1] Changed partition order.completed-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:44,649] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='order.completed', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition order.completed-0 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:44,651] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 0 become-follower partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:44,662] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 1 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:44,668] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition order.completed-0 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:44,668] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:44,687] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 for 1 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:44,691] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='order.completed', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 1 from controller 1 epoch 1 (state.change.logger)
[Kintellidine-inventory-service exited with code 1 (restarting)
intellidine-kafka                 | [2025-10-21 17:27:44,762] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition order.completed-0 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:44,766] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(order.completed-0) (kafka.server.ReplicaFetcherManager)
intellidine-kafka                 | [2025-10-21 17:27:44,767] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 1 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger)
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +59ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/payments}:[39m[38;5;3m +29ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/create-razorpay-order, POST} route[39m[38;5;3m +26ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/verify-razorpay, POST} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/confirm-cash, POST} route[39m[38;5;3m +3ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/health, GET} route[39m[38;5;3m +2ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/stats/daily, GET} route[39m[38;5;3m +3ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/:payment_id, GET} route[39m[38;5;3m +6ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:44 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments, GET} route[39m[38;5;3m +3ms[39m
intellidine-payment-service       | {"level":"WARN","timestamp":"2025-10-21T17:27:44.934Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
intellidine-kafka                 | [2025-10-21 17:27:44,977] INFO [LogLoader partition=order.completed-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:45,019] INFO Created log for partition order.completed-0 in /var/lib/kafka/data/order.completed-0 with properties {} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:45,032] INFO [Partition order.completed-0 broker=1] No checkpointed highwatermark is found for partition order.completed-0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:45,036] INFO [Partition order.completed-0 broker=1] Log loaded for partition order.completed-0 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:45,042] INFO [Broker id=1] Leader order.completed-0 with topic id Some(5mIbd_MQRrmhcbEnJzBgnw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:45 PM [32m    LOG[39m [38;5;3m[PaymentProducer] [39m[32mâ Kafka Producer (Payment Service) connected[39m
intellidine-payment-service       | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-payment-service       | Please manually install OpenSSL and try installing Prisma again.
intellidine-kafka                 | [2025-10-21 17:27:45,093] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition order.completed-0 (state.change.logger)
intellidine-payment-service       | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-payment-service       | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-payment-service       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-payment-service       | 
intellidine-payment-service       | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-payment-service       | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-kafka                 | [2025-10-21 17:27:45,121] INFO [Broker id=1] Finished LeaderAndIsr request in 434ms correlationId 1 from controller 1 for 1 partitions (state.change.logger)
intellidine-payment-service       | 
intellidine-payment-service       | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-payment-service       |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-payment-service       |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-payment-service       |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-payment-service       |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-payment-service       |     at async Proxy.onModuleInit (/app/dist/src/prisma.service.js:14:9)
intellidine-payment-service       |     at async Promise.all (index 1)
intellidine-payment-service       |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-payment-service       |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-payment-service       |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-payment-service       |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-payment-service       |   clientVersion: '5.22.0',
intellidine-payment-service       |   errorCode: undefined
intellidine-payment-service       | }
intellidine-payment-service       | 
intellidine-payment-service       | Node.js v20.19.5
intellidine-kafka                 | [2025-10-21 17:27:45,136] TRACE [Controller id=1 epoch=1] Received response LeaderAndIsrResponseData(errorCode=0, partitionErrors=[], topics=[LeaderAndIsrTopicError(topicId=5mIbd_MQRrmhcbEnJzBgnw, partitionErrors=[LeaderAndIsrPartitionError(topicName='', partitionIndex=0, errorCode=0)])]) for request LEADER_AND_ISR with correlation id 1 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:45,161] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='order.completed', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition order.completed-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:45,163] INFO [Broker id=1] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:45,166] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 2 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
[Kintellidine-payment-service exited with code 1 (restarting)
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:45 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:45 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +35ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:45 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +10ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:45 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +7ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:45 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, POST} route[39m[38;5;3m +2ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:45 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id, GET} route[39m[38;5;3m +2ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:45 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, GET} route[39m[38;5;3m +1ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:45 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/status, PATCH} route[39m[38;5;3m +4ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:45 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/cancel, PATCH} route[39m[38;5;3m +2ms[39m
intellidine-order-service         | {"level":"WARN","timestamp":"2025-10-21T17:27:45.666Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:45 PM [32m    LOG[39m [38;5;3m[KafkaProducerService] [39m[32mKafka producer connected[39m
intellidine-order-service         | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-order-service         | Please manually install OpenSSL and try installing Prisma again.
intellidine-order-service         | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-order-service         | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-order-service         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-order-service         | 
intellidine-order-service         | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-order-service         | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-order-service         | 
intellidine-order-service         | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-order-service         |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-order-service         |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-order-service         |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-order-service         |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-order-service         |     at async Proxy.onModuleInit (/app/dist/src/prisma.service.js:14:9)
intellidine-order-service         |     at async Promise.all (index 1)
intellidine-order-service         |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-order-service         |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-order-service         |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-order-service         |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-order-service         |   clientVersion: '5.22.0',
intellidine-order-service         |   errorCode: undefined
intellidine-order-service         | }
intellidine-order-service         | 
intellidine-order-service         | Node.js v20.19.5
[Kintellidine-order-service exited with code 1 (restarting)
intellidine-api-gateway           | node:internal/modules/cjs/loader:1210
intellidine-api-gateway           |   throw err;
intellidine-api-gateway           |   ^
intellidine-api-gateway           | 
intellidine-api-gateway           | Error: Cannot find module 'axios'
intellidine-api-gateway           | Require stack:
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/http.module.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/index.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/index.js
intellidine-api-gateway           | - /app/dist/src/app.module.js
intellidine-api-gateway           | - /app/dist/src/main.js
intellidine-api-gateway           |     at Module._resolveFilename (node:internal/modules/cjs/loader:1207:15)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1038:27)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19)
intellidine-api-gateway           |     at require (node:internal/modules/helpers:182:18)
intellidine-api-gateway           |     at Object.<anonymous> (/app/node_modules/@nestjs/axios/dist/http.module.js:16:33)
intellidine-api-gateway           |     at Module._compile (node:internal/modules/cjs/loader:1521:14)
intellidine-api-gateway           |     at Module._extensions..js (node:internal/modules/cjs/loader:1623:10)
intellidine-api-gateway           |     at Module.load (node:internal/modules/cjs/loader:1266:32)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1091:12)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19) {
intellidine-api-gateway           |   code: 'MODULE_NOT_FOUND',
intellidine-api-gateway           |   requireStack: [
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/http.module.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/index.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/index.js',
intellidine-api-gateway           |     '/app/dist/src/app.module.js',
intellidine-api-gateway           |     '/app/dist/src/main.js'
intellidine-api-gateway           |   ]
intellidine-api-gateway           | }
intellidine-api-gateway           | 
intellidine-api-gateway           | Node.js v20.19.5
intellidine-kafka                 | [2025-10-21 17:27:46,446] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:46,447] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:46,458] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 Map() (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:46,460] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[Kintellidine-api-gateway exited with code 1 (restarting)
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +71ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +15ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +7ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/request-otp, POST} route[39m[38;5;3m +1ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/verify-otp, POST} route[39m[38;5;3m +1ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/staff/login, POST} route[39m[38;5;3m +1ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +5ms[39m
intellidine-auth-service          | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-auth-service          | Please manually install OpenSSL and try installing Prisma again.
intellidine-auth-service          | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-auth-service          | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-auth-service          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-auth-service          | 
intellidine-auth-service          | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-auth-service          | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-auth-service          | 
intellidine-auth-service          | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-auth-service          |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-auth-service          |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-auth-service          |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599) {
intellidine-auth-service          |   clientVersion: '5.22.0',
intellidine-auth-service          |   errorCode: undefined
intellidine-auth-service          | }
intellidine-auth-service          | 
intellidine-auth-service          | Node.js v20.19.5
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +102ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +19ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +12ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu, GET} route[39m[38;5;3m +3ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/categories, GET} route[39m[38;5;3m +3ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, GET} route[39m[38;5;3m +2ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items, POST} route[39m[38;5;3m +4ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, PATCH} route[39m[38;5;3m +2ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, DELETE} route[39m[38;5;3m +2ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:47 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +8ms[39m
intellidine-menu-service          | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-menu-service          | Please manually install OpenSSL and try installing Prisma again.
intellidine-menu-service          | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-menu-service          | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-menu-service          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-menu-service          | 
intellidine-menu-service          | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-menu-service          | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-menu-service          | 
intellidine-menu-service          | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-menu-service          |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-menu-service          |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-menu-service          |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599) {
intellidine-menu-service          |   clientVersion: '5.22.0',
intellidine-menu-service          |   errorCode: undefined
intellidine-menu-service          | }
intellidine-menu-service          | 
intellidine-menu-service          | Node.js v20.19.5
[Kintellidine-auth-service exited with code 1 (restarting)
[Kintellidine-menu-service exited with code 1 (restarting)
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:48 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:48 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +36ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:48 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/inventory}:[39m[38;5;3m +10ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:48 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/health, GET} route[39m[38;5;3m +6ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:48 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, POST} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:48 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, GET} route[39m[38;5;3m +1ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:48 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, GET} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:48 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, PATCH} route[39m[38;5;3m +3ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:48 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/deduct, PATCH} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:48 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/alerts, GET} route[39m[38;5;3m +3ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:48 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/stats, GET} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | {"level":"INFO","timestamp":"2025-10-21T17:27:48.487Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"inventory-group"}
intellidine-inventory-service     | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-inventory-service     | Please manually install OpenSSL and try installing Prisma again.
intellidine-inventory-service     | [31m[Nest] 1  - [39m10/21/2025, 5:27:48 PM [31m  ERROR[39m [38;5;3m[InventoryService] [39m[31mâ Failed to start Inventory Service:[39m
intellidine-inventory-service     | [31m[Nest] 1  - [39m10/21/2025, 5:27:48 PM [31m  ERROR[39m [38;5;3m[InventoryService] [39m[31mPrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-inventory-service     | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-inventory-service     | 
intellidine-inventory-service     | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)[39m
intellidine-kafka                 | [2025-10-21 17:27:48,521] INFO Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(1), 1 -> ArrayBuffer(1), 2 -> ArrayBuffer(1), 3 -> ArrayBuffer(1), 4 -> ArrayBuffer(1), 5 -> ArrayBuffer(1), 6 -> ArrayBuffer(1), 7 -> ArrayBuffer(1), 8 -> ArrayBuffer(1), 9 -> ArrayBuffer(1), 10 -> ArrayBuffer(1), 11 -> ArrayBuffer(1), 12 -> ArrayBuffer(1), 13 -> ArrayBuffer(1), 14 -> ArrayBuffer(1), 15 -> ArrayBuffer(1), 16 -> ArrayBuffer(1), 17 -> ArrayBuffer(1), 18 -> ArrayBuffer(1), 19 -> ArrayBuffer(1), 20 -> ArrayBuffer(1), 21 -> ArrayBuffer(1), 22 -> ArrayBuffer(1), 23 -> ArrayBuffer(1), 24 -> ArrayBuffer(1), 25 -> ArrayBuffer(1), 26 -> ArrayBuffer(1), 27 -> ArrayBuffer(1), 28 -> ArrayBuffer(1), 29 -> ArrayBuffer(1), 30 -> ArrayBuffer(1), 31 -> ArrayBuffer(1), 32 -> ArrayBuffer(1), 33 -> ArrayBuffer(1), 34 -> ArrayBuffer(1), 35 -> ArrayBuffer(1), 36 -> ArrayBuffer(1), 37 -> ArrayBuffer(1), 38 -> ArrayBuffer(1), 39 -> ArrayBuffer(1), 40 -> ArrayBuffer(1), 41 -> ArrayBuffer(1), 42 -> ArrayBuffer(1), 43 -> ArrayBuffer(1), 44 -> ArrayBuffer(1), 45 -> ArrayBuffer(1), 46 -> ArrayBuffer(1), 47 -> ArrayBuffer(1), 48 -> ArrayBuffer(1), 49 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
intellidine-kafka                 | [2025-10-21 17:27:48,573] INFO [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(3cMdvSJLQKqVWBsXQKQOnQ),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:48,574] INFO [Controller id=1] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:48,576] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,576] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,577] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,578] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,579] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,584] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,585] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,586] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,587] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,591] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,592] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,592] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,592] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,592] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,592] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,592] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,592] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,593] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,593] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,593] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,593] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,593] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,595] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,596] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,597] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,597] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,598] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,599] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,599] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,600] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,601] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,602] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,603] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,604] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,604] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,604] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,604] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,605] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,605] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,605] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,605] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,605] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,605] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,605] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,605] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,605] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,606] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,606] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,606] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,606] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,606] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,610] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-32 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,610] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-5 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,610] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-44 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,611] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-48 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,612] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-46 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,612] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-20 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,612] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-43 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,612] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-24 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,613] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-6 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,613] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-18 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,613] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-21 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,614] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-1 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,614] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-14 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,615] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-34 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,616] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-16 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,616] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-29 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,616] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-11 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,616] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-0 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,617] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-22 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,617] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-47 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,617] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-36 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,617] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-28 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,617] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-42 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,617] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-9 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,618] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-37 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,618] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-13 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,618] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-30 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,618] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-35 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,618] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-39 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,618] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-12 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,619] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-27 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,619] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-45 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,619] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-19 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,619] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-49 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,619] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-40 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,619] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-41 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,619] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-38 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,621] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-8 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,621] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-7 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,621] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-33 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,621] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-25 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,621] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-31 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,621] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-23 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,622] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-10 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,622] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-2 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,622] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-17 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,622] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-4 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,622] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-15 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,622] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-26 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,622] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-3 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:48,623] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[Kintellidine-inventory-service exited with code 1 (restarting)
intellidine-kafka                 | [2025-10-21 17:27:49,049] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,050] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,051] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,051] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,051] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,051] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,051] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,052] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,052] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,052] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,052] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,053] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,053] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,053] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,053] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,053] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,054] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,054] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,054] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,054] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,055] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,055] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,055] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,055] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,056] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,056] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,056] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,056] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,057] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,057] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,057] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,057] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,057] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,058] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,058] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,058] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,058] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,058] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,059] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,059] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,059] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,059] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,060] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,060] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,060] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,060] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,060] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,061] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,061] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,061] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,061] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-13 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,062] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-46 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,062] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-9 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,062] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-42 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,063] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-21 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,064] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-17 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,064] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-30 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,064] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-26 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,064] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-5 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,065] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-38 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,065] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,065] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-34 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,065] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-16 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,065] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-45 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,066] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-12 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,066] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-41 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,066] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-24 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,066] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-20 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,066] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-49 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,067] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-0 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,067] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-29 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,067] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-25 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,067] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-8 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,068] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-37 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,068] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,068] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-33 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,068] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-15 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,068] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-48 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,069] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-11 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,069] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-44 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,069] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-23 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,069] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-19 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,070] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-32 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,070] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-28 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,070] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-7 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,070] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-40 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,071] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-3 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,071] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-36 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,071] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-47 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,072] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-14 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,072] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-43 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,072] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-10 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,072] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-22 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,073] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-18 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,073] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-31 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,073] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-27 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,073] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-39 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,073] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-6 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,074] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-35 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,074] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-2 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,074] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 50 become-leader and 0 become-follower partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,076] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 50 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,080] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-32 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,081] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-5 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,082] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-44 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,081] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 for 50 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,082] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-48 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,083] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,083] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-46 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,084] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-20 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,084] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-43 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,085] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-24 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,083] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,086] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-6 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,086] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-18 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,086] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,087] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-21 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,087] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,087] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-1 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,089] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-14 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,088] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,089] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,089] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,089] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-34 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,089] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-16 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,090] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-29 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,090] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-11 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,090] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-0 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,090] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-22 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,089] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,092] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-47 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,092] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,092] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-36 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,092] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,092] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-28 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,092] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,092] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-42 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,092] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,092] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-9 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,092] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-37 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,092] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-13 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,092] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-30 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,093] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-35 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,094] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-39 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,094] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-12 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,092] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,094] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,094] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-27 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,095] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-45 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,094] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,095] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-19 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,095] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,095] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-49 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,095] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,096] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-40 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,096] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,096] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-41 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,096] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,096] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-38 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,096] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,096] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-8 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,096] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,096] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-7 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,097] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-33 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,096] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,098] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,098] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,098] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,098] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,098] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,099] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,098] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-25 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,100] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,100] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,100] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,100] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,100] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,100] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-31 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-23 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-10 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-2 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-17 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-4 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-15 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,100] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,103] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,103] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,103] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-26 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,104] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-3 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,104] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,104] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,105] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,105] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,105] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,105] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,105] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,105] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,105] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,105] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,106] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,106] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,106] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,106] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,154] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,154] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,154] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-41 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,154] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,154] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,154] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,154] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,154] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,155] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,155] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-11 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,155] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-26 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,155] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,155] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,155] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,155] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,155] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,155] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,155] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,155] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,155] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-2 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,155] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,156] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,156] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-47 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,156] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-17 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,156] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-32 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,156] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,156] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,156] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,156] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-29 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,157] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-44 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,157] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-14 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,157] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-23 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,157] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-38 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,157] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-8 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,157] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,157] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,157] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,157] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,157] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-35 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,158] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-5 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,158] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-20 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,158] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,158] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,158] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,158] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,158] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,158] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,158] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,158] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,158] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,160] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
intellidine-kafka                 | [2025-10-21 17:27:49,161] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 1 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,177] INFO [LogLoader partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,179] INFO Created log for partition __consumer_offsets-3 in /var/lib/kafka/data/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,180] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,180] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,182] INFO [Broker id=1] Leader __consumer_offsets-3 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,207] INFO [LogLoader partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,209] INFO Created log for partition __consumer_offsets-18 in /var/lib/kafka/data/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,209] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,210] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,210] INFO [Broker id=1] Leader __consumer_offsets-18 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-kafka                 | [2025-10-21 17:27:49,243] INFO [LogLoader partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,246] INFO Created log for partition __consumer_offsets-41 in /var/lib/kafka/data/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,247] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,247] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,247] INFO [Broker id=1] Leader __consumer_offsets-41 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +56ms[39m
intellidine-kafka                 | [2025-10-21 17:27:49,269] INFO [LogLoader partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,271] INFO Created log for partition __consumer_offsets-10 in /var/lib/kafka/data/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,271] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,272] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,272] INFO [Broker id=1] Leader __consumer_offsets-10 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/analytics}:[39m[38;5;3m +26ms[39m
intellidine-kafka                 | [2025-10-21 17:27:49,300] INFO [LogLoader partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,301] INFO Created log for partition __consumer_offsets-33 in /var/lib/kafka/data/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/health, GET} route[39m[38;5;3m +11ms[39m
intellidine-kafka                 | [2025-10-21 17:27:49,301] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,302] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,302] INFO [Broker id=1] Leader __consumer_offsets-33 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/daily, GET} route[39m[38;5;3m +2ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/recent, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/range, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/aggregated, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/orders/history, GET} route[39m[38;5;3m +2ms[39m
intellidine-kafka                 | [2025-10-21 17:27:49,326] INFO [LogLoader partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,330] INFO Created log for partition __consumer_offsets-48 in /var/lib/kafka/data/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,330] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,331] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,331] INFO [Broker id=1] Leader __consumer_offsets-48 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-kafka                 | [2025-10-21 17:27:49,360] INFO [LogLoader partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,364] INFO Created log for partition __consumer_offsets-19 in /var/lib/kafka/data/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,364] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,365] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,365] INFO [Broker id=1] Leader __consumer_offsets-19 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,383] INFO [LogLoader partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,386] INFO Created log for partition __consumer_offsets-34 in /var/lib/kafka/data/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,387] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,387] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,388] INFO [Broker id=1] Leader __consumer_offsets-34 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[KafkaConsumerService] [39m[32mKafka consumer connected[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +45ms[39m
intellidine-kafka                 | [2025-10-21 17:27:49,410] INFO Creating topic orders with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
intellidine-kafka                 | [2025-10-21 17:27:49,413] INFO [LogLoader partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,415] INFO Created log for partition __consumer_offsets-4 in /var/lib/kafka/data/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,416] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,417] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,417] INFO [Broker id=1] Leader __consumer_offsets-4 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/payments}:[39m[38;5;3m +28ms[39m
intellidine-analytics-service     | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-analytics-service     | Please manually install OpenSSL and try installing Prisma again.
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/create-razorpay-order, POST} route[39m[38;5;3m +21ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/verify-razorpay, POST} route[39m[38;5;3m +2ms[39m
intellidine-analytics-service     | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-analytics-service     | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-analytics-service     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/confirm-cash, POST} route[39m[38;5;3m +2ms[39m
intellidine-analytics-service     | 
intellidine-analytics-service     | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-analytics-service     | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/health, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | 
intellidine-analytics-service     | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-analytics-service     |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-analytics-service     |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-analytics-service     |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-analytics-service     |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-analytics-service     |     at async Proxy.onModuleInit (/app/dist/prisma.service.js:19:9)
intellidine-analytics-service     |     at async Promise.all (index 0)
intellidine-analytics-service     |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-analytics-service     |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-analytics-service     |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-analytics-service     |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-analytics-service     |   clientVersion: '5.22.0',
intellidine-analytics-service     |   errorCode: undefined
intellidine-analytics-service     | }
intellidine-analytics-service     | 
intellidine-analytics-service     | Node.js v20.19.5
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/stats/daily, GET} route[39m[38;5;3m +2ms[39m
intellidine-kafka                 | [2025-10-21 17:27:49,454] INFO [LogLoader partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/:payment_id, GET} route[39m[38;5;3m +5ms[39m
intellidine-kafka                 | [2025-10-21 17:27:49,459] INFO Created log for partition __consumer_offsets-11 in /var/lib/kafka/data/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments, GET} route[39m[38;5;3m +3ms[39m
intellidine-kafka                 | [2025-10-21 17:27:49,456] INFO [Controller id=1] New topics: [Set(orders)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(orders,Some(hy5_yQ_lSlG0T0CwxGgkZw),Map(orders-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:49,460] INFO [Controller id=1] New partition creation callback for orders-0 (kafka.controller.KafkaController)
intellidine-kafka                 | [2025-10-21 17:27:49,461] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,461] INFO [Controller id=1 epoch=1] Changed partition orders-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,462] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,462] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,464] INFO [Broker id=1] Leader __consumer_offsets-11 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,465] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition orders-0 from NonExistentReplica to NewReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,465] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
intellidine-payment-service       | {"level":"WARN","timestamp":"2025-10-21T17:27:49.469Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
intellidine-kafka                 | [2025-10-21 17:27:49,501] INFO [Controller id=1 epoch=1] Changed partition orders-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,502] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='orders', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition orders-0 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,503] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 0 become-follower partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,504] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 1 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,507] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition orders-0 from NewReplica to OnlineReplica (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,509] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,519] INFO [LogLoader partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,524] INFO Created log for partition __consumer_offsets-26 in /var/lib/kafka/data/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,529] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,529] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,531] INFO [Broker id=1] Leader __consumer_offsets-26 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,560] INFO [LogLoader partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,562] INFO Created log for partition __consumer_offsets-49 in /var/lib/kafka/data/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,562] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,562] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,564] INFO [Broker id=1] Leader __consumer_offsets-49 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,590] INFO [LogLoader partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,592] INFO Created log for partition __consumer_offsets-39 in /var/lib/kafka/data/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,593] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,593] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,593] INFO [Broker id=1] Leader __consumer_offsets-39 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:49 PM [32m    LOG[39m [38;5;3m[PaymentProducer] [39m[32mâ Kafka Producer (Payment Service) connected[39m
intellidine-payment-service       | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-payment-service       | Please manually install OpenSSL and try installing Prisma again.
intellidine-kafka                 | [2025-10-21 17:27:49,624] INFO [LogLoader partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,626] INFO Created log for partition __consumer_offsets-9 in /var/lib/kafka/data/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,626] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,626] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,626] INFO [Broker id=1] Leader __consumer_offsets-9 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-payment-service       | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-payment-service       | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-payment-service       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-payment-service       | 
intellidine-payment-service       | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-payment-service       | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-payment-service       | 
intellidine-payment-service       | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-payment-service       |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-payment-service       |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-payment-service       |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-payment-service       |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-payment-service       |     at async Proxy.onModuleInit (/app/dist/src/prisma.service.js:14:9)
intellidine-payment-service       |     at async Promise.all (index 1)
intellidine-payment-service       |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-payment-service       |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-payment-service       |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-payment-service       |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-payment-service       |   clientVersion: '5.22.0',
intellidine-payment-service       |   errorCode: undefined
intellidine-payment-service       | }
intellidine-payment-service       | 
intellidine-payment-service       | Node.js v20.19.5
intellidine-kafka                 | [2025-10-21 17:27:49,650] INFO [LogLoader partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,653] INFO Created log for partition __consumer_offsets-24 in /var/lib/kafka/data/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,653] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,653] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,654] INFO [Broker id=1] Leader __consumer_offsets-24 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,679] INFO [LogLoader partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,681] INFO Created log for partition __consumer_offsets-31 in /var/lib/kafka/data/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,681] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,681] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,682] INFO [Broker id=1] Leader __consumer_offsets-31 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,711] INFO [LogLoader partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,712] INFO Created log for partition __consumer_offsets-46 in /var/lib/kafka/data/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,712] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,713] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,713] INFO [Broker id=1] Leader __consumer_offsets-46 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,736] INFO [LogLoader partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,738] INFO Created log for partition __consumer_offsets-1 in /var/lib/kafka/data/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,738] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,738] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,739] INFO [Broker id=1] Leader __consumer_offsets-1 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,760] INFO [LogLoader partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,761] INFO Created log for partition __consumer_offsets-16 in /var/lib/kafka/data/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,762] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,762] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,763] INFO [Broker id=1] Leader __consumer_offsets-16 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,785] INFO [LogLoader partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,787] INFO Created log for partition __consumer_offsets-2 in /var/lib/kafka/data/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,787] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,787] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,787] INFO [Broker id=1] Leader __consumer_offsets-2 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,819] INFO [LogLoader partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,822] INFO Created log for partition __consumer_offsets-25 in /var/lib/kafka/data/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,823] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,823] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,823] INFO [Broker id=1] Leader __consumer_offsets-25 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,844] INFO [LogLoader partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,846] INFO Created log for partition __consumer_offsets-40 in /var/lib/kafka/data/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,846] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,846] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,847] INFO [Broker id=1] Leader __consumer_offsets-40 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,892] INFO [LogLoader partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,896] INFO Created log for partition __consumer_offsets-47 in /var/lib/kafka/data/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,896] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,896] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,897] INFO [Broker id=1] Leader __consumer_offsets-47 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,923] INFO [LogLoader partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,928] INFO Created log for partition __consumer_offsets-17 in /var/lib/kafka/data/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,929] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,930] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,930] INFO [Broker id=1] Leader __consumer_offsets-17 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:49,968] INFO [LogLoader partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:49,970] INFO Created log for partition __consumer_offsets-32 in /var/lib/kafka/data/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:49,970] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,971] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:49,971] INFO [Broker id=1] Leader __consumer_offsets-32 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,004] INFO [LogLoader partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,006] INFO Created log for partition __consumer_offsets-37 in /var/lib/kafka/data/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,007] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,007] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,007] INFO [Broker id=1] Leader __consumer_offsets-37 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
[Kintellidine-analytics-service exited with code 1 (restarting)
intellidine-kafka                 | [2025-10-21 17:27:50,032] INFO [LogLoader partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,035] INFO Created log for partition __consumer_offsets-7 in /var/lib/kafka/data/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,035] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,035] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,036] INFO [Broker id=1] Leader __consumer_offsets-7 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,059] INFO [LogLoader partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,061] INFO Created log for partition __consumer_offsets-22 in /var/lib/kafka/data/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,061] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,061] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,062] INFO [Broker id=1] Leader __consumer_offsets-22 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,088] INFO [LogLoader partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,090] INFO Created log for partition __consumer_offsets-29 in /var/lib/kafka/data/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,090] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,090] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,090] INFO [Broker id=1] Leader __consumer_offsets-29 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
[Kintellidine-payment-service exited with code 1 (restarting)
intellidine-kafka                 | [2025-10-21 17:27:50,114] INFO [LogLoader partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,115] INFO Created log for partition __consumer_offsets-44 in /var/lib/kafka/data/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,115] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,116] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,116] INFO [Broker id=1] Leader __consumer_offsets-44 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,137] INFO [LogLoader partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,139] INFO Created log for partition __consumer_offsets-14 in /var/lib/kafka/data/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,139] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,139] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,140] INFO [Broker id=1] Leader __consumer_offsets-14 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,160] INFO [LogLoader partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,163] INFO Created log for partition __consumer_offsets-23 in /var/lib/kafka/data/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,163] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,163] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,164] INFO [Broker id=1] Leader __consumer_offsets-23 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,193] INFO [LogLoader partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,197] INFO Created log for partition __consumer_offsets-38 in /var/lib/kafka/data/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,199] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,200] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,203] INFO [Broker id=1] Leader __consumer_offsets-38 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,225] INFO [LogLoader partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,227] INFO Created log for partition __consumer_offsets-8 in /var/lib/kafka/data/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,228] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,228] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,228] INFO [Broker id=1] Leader __consumer_offsets-8 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,250] INFO [LogLoader partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,252] INFO Created log for partition __consumer_offsets-45 in /var/lib/kafka/data/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,252] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,252] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,253] INFO [Broker id=1] Leader __consumer_offsets-45 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,271] INFO [LogLoader partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,274] INFO Created log for partition __consumer_offsets-15 in /var/lib/kafka/data/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,274] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,274] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,275] INFO [Broker id=1] Leader __consumer_offsets-15 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:50 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-kafka                 | [2025-10-21 17:27:50,294] INFO [LogLoader partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,295] INFO Created log for partition __consumer_offsets-30 in /var/lib/kafka/data/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,296] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,296] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,296] INFO [Broker id=1] Leader __consumer_offsets-30 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,316] INFO [LogLoader partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,317] INFO Created log for partition __consumer_offsets-0 in /var/lib/kafka/data/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,317] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,318] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,318] INFO [Broker id=1] Leader __consumer_offsets-0 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:50 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +44ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:50 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +14ms[39m
intellidine-kafka                 | [2025-10-21 17:27:50,339] INFO [LogLoader partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,342] INFO Created log for partition __consumer_offsets-35 in /var/lib/kafka/data/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,342] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,342] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,342] INFO [Broker id=1] Leader __consumer_offsets-35 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:50 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +9ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:50 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, POST} route[39m[38;5;3m +2ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:50 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id, GET} route[39m[38;5;3m +2ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:50 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, GET} route[39m[38;5;3m +1ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:50 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/status, PATCH} route[39m[38;5;3m +6ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:50 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/cancel, PATCH} route[39m[38;5;3m +3ms[39m
intellidine-kafka                 | [2025-10-21 17:27:50,362] INFO [LogLoader partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,364] INFO Created log for partition __consumer_offsets-5 in /var/lib/kafka/data/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,364] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,365] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,365] INFO [Broker id=1] Leader __consumer_offsets-5 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-order-service         | {"level":"WARN","timestamp":"2025-10-21T17:27:50.373Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
intellidine-kafka                 | [2025-10-21 17:27:50,384] INFO [LogLoader partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,386] INFO Created log for partition __consumer_offsets-20 in /var/lib/kafka/data/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,386] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,387] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,387] INFO [Broker id=1] Leader __consumer_offsets-20 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,405] INFO [LogLoader partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,407] INFO Created log for partition __consumer_offsets-27 in /var/lib/kafka/data/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,407] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,407] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,407] INFO [Broker id=1] Leader __consumer_offsets-27 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,425] INFO [LogLoader partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,426] INFO Created log for partition __consumer_offsets-42 in /var/lib/kafka/data/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,426] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,426] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,427] INFO [Broker id=1] Leader __consumer_offsets-42 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,445] INFO [LogLoader partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,446] INFO Created log for partition __consumer_offsets-12 in /var/lib/kafka/data/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,447] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,447] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,447] INFO [Broker id=1] Leader __consumer_offsets-12 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:50 PM [32m    LOG[39m [38;5;3m[KafkaProducerService] [39m[32mKafka producer connected[39m
intellidine-kafka                 | [2025-10-21 17:27:50,467] INFO [LogLoader partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,469] INFO Created log for partition __consumer_offsets-21 in /var/lib/kafka/data/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,469] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,469] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,469] INFO [Broker id=1] Leader __consumer_offsets-21 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-order-service         | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-order-service         | Please manually install OpenSSL and try installing Prisma again.
intellidine-kafka                 | [2025-10-21 17:27:50,492] INFO [LogLoader partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,493] INFO Created log for partition __consumer_offsets-36 in /var/lib/kafka/data/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,493] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,494] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,494] INFO [Broker id=1] Leader __consumer_offsets-36 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-order-service         | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-order-service         | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-order-service         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-order-service         | 
intellidine-order-service         | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-order-service         | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-order-service         | 
intellidine-order-service         | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-order-service         |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-order-service         |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-order-service         |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-order-service         |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-order-service         |     at async Proxy.onModuleInit (/app/dist/src/prisma.service.js:14:9)
intellidine-order-service         |     at async Promise.all (index 1)
intellidine-order-service         |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-order-service         |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-order-service         |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-order-service         |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-order-service         |   clientVersion: '5.22.0',
intellidine-order-service         |   errorCode: undefined
intellidine-order-service         | }
intellidine-order-service         | 
intellidine-order-service         | Node.js v20.19.5
intellidine-kafka                 | [2025-10-21 17:27:50,516] INFO [LogLoader partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,518] INFO Created log for partition __consumer_offsets-6 in /var/lib/kafka/data/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,519] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,519] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,520] INFO [Broker id=1] Leader __consumer_offsets-6 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,544] INFO [LogLoader partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,548] INFO Created log for partition __consumer_offsets-43 in /var/lib/kafka/data/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,549] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,550] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,550] INFO [Broker id=1] Leader __consumer_offsets-43 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,570] INFO [LogLoader partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,573] INFO Created log for partition __consumer_offsets-13 in /var/lib/kafka/data/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,574] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,574] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,574] INFO [Broker id=1] Leader __consumer_offsets-13 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,592] INFO [LogLoader partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,594] INFO Created log for partition __consumer_offsets-28 in /var/lib/kafka/data/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,594] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,594] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,595] INFO [Broker id=1] Leader __consumer_offsets-28 with topic id Some(3cMdvSJLQKqVWBsXQKQOnQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,610] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,611] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,611] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-41 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,611] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,611] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,611] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,611] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,611] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,611] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,611] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-11 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-26 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-2 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-47 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-17 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-32 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-29 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-44 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-14 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-23 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-38 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-8 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-35 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-5 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-20 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,612] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,615] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,618] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,621] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,621] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,621] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,621] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,621] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,621] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,621] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,622] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,622] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,622] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,622] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,622] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,622] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,622] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,622] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,622] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,622] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,622] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,622] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,622] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,623] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,624] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,625] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,625] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,625] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,625] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,625] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,625] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,625] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,625] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,625] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,625] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,625] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,625] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,626] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,626] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,626] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,626] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,626] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,626] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,626] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,626] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,626] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,626] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,626] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,626] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,627] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
intellidine-kafka                 | [2025-10-21 17:27:50,628] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,628] INFO [Broker id=1] Finished LeaderAndIsr request in 1547ms correlationId 3 from controller 1 for 50 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,631] TRACE [Controller id=1 epoch=1] Received response LeaderAndIsrResponseData(errorCode=0, partitionErrors=[], topics=[LeaderAndIsrTopicError(topicId=3cMdvSJLQKqVWBsXQKQOnQ, partitionErrors=[LeaderAndIsrPartitionError(topicName='', partitionIndex=13, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=46, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=9, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=42, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=21, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=17, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=30, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=26, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=5, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=38, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=1, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=34, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=16, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=45, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=12, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=41, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=24, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=20, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=49, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=0, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=29, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=25, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=8, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=37, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=4, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=33, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=15, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=48, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=11, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=44, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=23, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=19, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=32, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=28, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=7, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=40, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=3, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=36, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=47, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=14, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=43, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=10, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=22, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=18, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=31, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=27, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=39, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=6, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=35, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=2, errorCode=0)])]) for request LEADER_AND_ISR with correlation id 3 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,635] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-13 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,635] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-46 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,635] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-9 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,636] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-42 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,636] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-21 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,636] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-17 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,636] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-30 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,636] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-26 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,636] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-5 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,636] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-38 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,636] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-34 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-16 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-45 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-12 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-41 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-24 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-20 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-49 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-29 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-25 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-8 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-37 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-33 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-15 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-48 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-11 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-44 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-23 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,637] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-19 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-32 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-28 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-7 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-40 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-36 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-47 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-14 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-43 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-10 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-22 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-18 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-31 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-27 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-39 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-6 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-35 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,638] INFO [Broker id=1] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,640] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 4 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,642] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 for 1 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,643] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='orders', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,643] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 22 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,645] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 23 milliseconds for epoch 0, of which 23 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,645] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition orders-0 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,645] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 24 milliseconds for epoch 0, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,645] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(orders-0) (kafka.server.ReplicaFetcherManager)
intellidine-kafka                 | [2025-10-21 17:27:50,645] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 1 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,646] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 25 milliseconds for epoch 0, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,646] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 24 milliseconds for epoch 0, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,647] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 25 milliseconds for epoch 0, of which 25 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,648] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 26 milliseconds for epoch 0, of which 25 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,648] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 26 milliseconds for epoch 0, of which 26 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,649] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 26 milliseconds for epoch 0, of which 26 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,649] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 27 milliseconds for epoch 0, of which 27 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,649] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 27 milliseconds for epoch 0, of which 27 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,650] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 27 milliseconds for epoch 0, of which 27 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,650] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 27 milliseconds for epoch 0, of which 27 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,651] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 28 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,651] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 28 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,652] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 29 milliseconds for epoch 0, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,653] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 29 milliseconds for epoch 0, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,653] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,653] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,654] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,654] INFO [LogLoader partition=orders-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
intellidine-kafka                 | [2025-10-21 17:27:50,654] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,655] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 31 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,655] INFO Created log for partition orders-0 in /var/lib/kafka/data/orders-0 with properties {} (kafka.log.LogManager)
intellidine-kafka                 | [2025-10-21 17:27:50,656] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 32 milliseconds for epoch 0, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,657] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 33 milliseconds for epoch 0, of which 33 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,658] INFO [Partition orders-0 broker=1] No checkpointed highwatermark is found for partition orders-0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,658] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 34 milliseconds for epoch 0, of which 33 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,658] INFO [Partition orders-0 broker=1] Log loaded for partition orders-0 with initial high watermark 0 (kafka.cluster.Partition)
intellidine-kafka                 | [2025-10-21 17:27:50,658] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 34 milliseconds for epoch 0, of which 34 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,658] INFO [Broker id=1] Leader orders-0 with topic id Some(hy5_yQ_lSlG0T0CwxGgkZw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader epoch was -1. (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,659] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 35 milliseconds for epoch 0, of which 34 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,659] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 34 milliseconds for epoch 0, of which 34 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,659] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 34 milliseconds for epoch 0, of which 34 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,660] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 35 milliseconds for epoch 0, of which 35 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,660] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 35 milliseconds for epoch 0, of which 35 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,661] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 35 milliseconds for epoch 0, of which 35 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,661] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 36 milliseconds for epoch 0, of which 36 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,662] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 37 milliseconds for epoch 0, of which 36 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,662] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 36 milliseconds for epoch 0, of which 36 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,662] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 36 milliseconds for epoch 0, of which 36 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,663] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 37 milliseconds for epoch 0, of which 37 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,663] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 37 milliseconds for epoch 0, of which 37 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,664] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 38 milliseconds for epoch 0, of which 38 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,665] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 38 milliseconds for epoch 0, of which 37 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,665] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 38 milliseconds for epoch 0, of which 38 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,665] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 38 milliseconds for epoch 0, of which 38 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,666] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 39 milliseconds for epoch 0, of which 38 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,666] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 39 milliseconds for epoch 0, of which 39 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,666] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 39 milliseconds for epoch 0, of which 39 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,667] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 40 milliseconds for epoch 0, of which 40 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,667] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 40 milliseconds for epoch 0, of which 40 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,668] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 41 milliseconds for epoch 0, of which 40 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,668] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 41 milliseconds for epoch 0, of which 41 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,668] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 40 milliseconds for epoch 0, of which 40 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
intellidine-kafka                 | [2025-10-21 17:27:50,670] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition orders-0 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,672] INFO [Broker id=1] Finished LeaderAndIsr request in 30ms correlationId 5 from controller 1 for 1 partitions (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,673] TRACE [Controller id=1 epoch=1] Received response LeaderAndIsrResponseData(errorCode=0, partitionErrors=[], topics=[LeaderAndIsrTopicError(topicId=hy5_yQ_lSlG0T0CwxGgkZw, partitionErrors=[LeaderAndIsrPartitionError(topicName='', partitionIndex=0, errorCode=0)])]) for request LEADER_AND_ISR with correlation id 5 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,675] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='orders', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition orders-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,676] INFO [Broker id=1] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
intellidine-kafka                 | [2025-10-21 17:27:50,678] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 6 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
[Kintellidine-order-service exited with code 1 (restarting)
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +44ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +11ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +4ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/request-otp, POST} route[39m[38;5;3m +1ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/verify-otp, POST} route[39m[38;5;3m +1ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/staff/login, POST} route[39m[38;5;3m +1ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +3ms[39m
intellidine-auth-service          | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-auth-service          | Please manually install OpenSSL and try installing Prisma again.
intellidine-auth-service          | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-auth-service          | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-auth-service          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-auth-service          | 
intellidine-auth-service          | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-auth-service          | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-auth-service          | 
intellidine-auth-service          | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-auth-service          |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-auth-service          |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-auth-service          |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599) {
intellidine-auth-service          |   clientVersion: '5.22.0',
intellidine-auth-service          |   errorCode: undefined
intellidine-auth-service          | }
intellidine-auth-service          | 
intellidine-auth-service          | Node.js v20.19.5
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +52ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +7ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +5ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu, GET} route[39m[38;5;3m +0ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/categories, GET} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, GET} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items, POST} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, PATCH} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, DELETE} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:51 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +3ms[39m
intellidine-menu-service          | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-menu-service          | Please manually install OpenSSL and try installing Prisma again.
intellidine-menu-service          | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-menu-service          | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-menu-service          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-menu-service          | 
intellidine-menu-service          | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-menu-service          | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-menu-service          | 
intellidine-menu-service          | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-menu-service          |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-menu-service          |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-menu-service          |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599) {
intellidine-menu-service          |   clientVersion: '5.22.0',
intellidine-menu-service          |   errorCode: undefined
intellidine-menu-service          | }
intellidine-menu-service          | 
intellidine-menu-service          | Node.js v20.19.5
[Kintellidine-auth-service exited with code 1 (restarting)
[Kintellidine-menu-service exited with code 1 (restarting)
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:53 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:53 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +43ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:53 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/inventory}:[39m[38;5;3m +10ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:53 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/health, GET} route[39m[38;5;3m +6ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:53 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, POST} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:53 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, GET} route[39m[38;5;3m +1ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:53 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, GET} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:53 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, PATCH} route[39m[38;5;3m +3ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:53 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/deduct, PATCH} route[39m[38;5;3m +1ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:53 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/alerts, GET} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:27:53 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/stats, GET} route[39m[38;5;3m +1ms[39m
intellidine-inventory-service     | {"level":"INFO","timestamp":"2025-10-21T17:27:53.261Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"inventory-group"}
intellidine-inventory-service     | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-inventory-service     | Please manually install OpenSSL and try installing Prisma again.
intellidine-inventory-service     | [31m[Nest] 1  - [39m10/21/2025, 5:27:53 PM [31m  ERROR[39m [38;5;3m[InventoryService] [39m[31mâ Failed to start Inventory Service:[39m
intellidine-inventory-service     | [31m[Nest] 1  - [39m10/21/2025, 5:27:53 PM [31m  ERROR[39m [38;5;3m[InventoryService] [39m[31mPrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-inventory-service     | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-inventory-service     | 
intellidine-inventory-service     | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)[39m
[Kintellidine-inventory-service exited with code 1 (restarting)
intellidine-api-gateway           | node:internal/modules/cjs/loader:1210
intellidine-api-gateway           |   throw err;
intellidine-api-gateway           |   ^
intellidine-api-gateway           | 
intellidine-api-gateway           | Error: Cannot find module 'axios'
intellidine-api-gateway           | Require stack:
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/http.module.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/index.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/index.js
intellidine-api-gateway           | - /app/dist/src/app.module.js
intellidine-api-gateway           | - /app/dist/src/main.js
intellidine-api-gateway           |     at Module._resolveFilename (node:internal/modules/cjs/loader:1207:15)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1038:27)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19)
intellidine-api-gateway           |     at require (node:internal/modules/helpers:182:18)
intellidine-api-gateway           |     at Object.<anonymous> (/app/node_modules/@nestjs/axios/dist/http.module.js:16:33)
intellidine-api-gateway           |     at Module._compile (node:internal/modules/cjs/loader:1521:14)
intellidine-api-gateway           |     at Module._extensions..js (node:internal/modules/cjs/loader:1623:10)
intellidine-api-gateway           |     at Module.load (node:internal/modules/cjs/loader:1266:32)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1091:12)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19) {
intellidine-api-gateway           |   code: 'MODULE_NOT_FOUND',
intellidine-api-gateway           |   requireStack: [
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/http.module.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/index.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/index.js',
intellidine-api-gateway           |     '/app/dist/src/app.module.js',
intellidine-api-gateway           |     '/app/dist/src/main.js'
intellidine-api-gateway           |   ]
intellidine-api-gateway           | }
intellidine-api-gateway           | 
intellidine-api-gateway           | Node.js v20.19.5
[Kintellidine-api-gateway exited with code 1 (restarting)
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:54 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:54 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +23ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:54 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/payments}:[39m[38;5;3m +13ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:54 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/create-razorpay-order, POST} route[39m[38;5;3m +9ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:54 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/verify-razorpay, POST} route[39m[38;5;3m +0ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:54 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/confirm-cash, POST} route[39m[38;5;3m +1ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:54 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/health, GET} route[39m[38;5;3m +0ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:54 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/stats/daily, GET} route[39m[38;5;3m +1ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:54 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/:payment_id, GET} route[39m[38;5;3m +1ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:54 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments, GET} route[39m[38;5;3m +1ms[39m
intellidine-payment-service       | {"level":"WARN","timestamp":"2025-10-21T17:27:54.601Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:27:54 PM [32m    LOG[39m [38;5;3m[PaymentProducer] [39m[32mâ Kafka Producer (Payment Service) connected[39m
intellidine-payment-service       | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-payment-service       | Please manually install OpenSSL and try installing Prisma again.
intellidine-payment-service       | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-payment-service       | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-payment-service       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-payment-service       | 
intellidine-payment-service       | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-payment-service       | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-payment-service       | 
intellidine-payment-service       | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-payment-service       |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-payment-service       |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-payment-service       |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-payment-service       |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-payment-service       |     at async Proxy.onModuleInit (/app/dist/src/prisma.service.js:14:9)
intellidine-payment-service       |     at async Promise.all (index 1)
intellidine-payment-service       |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-payment-service       |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-payment-service       |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-payment-service       |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-payment-service       |   clientVersion: '5.22.0',
intellidine-payment-service       |   errorCode: undefined
intellidine-payment-service       | }
intellidine-payment-service       | 
intellidine-payment-service       | Node.js v20.19.5
[Kintellidine-payment-service exited with code 1 (restarting)
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:55 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:55 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +29ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:55 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +9ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:55 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +6ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:55 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, POST} route[39m[38;5;3m +1ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:55 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id, GET} route[39m[38;5;3m +1ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:55 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, GET} route[39m[38;5;3m +1ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:55 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/status, PATCH} route[39m[38;5;3m +3ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:55 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/cancel, PATCH} route[39m[38;5;3m +2ms[39m
intellidine-order-service         | {"level":"WARN","timestamp":"2025-10-21T17:27:55.210Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:27:55 PM [32m    LOG[39m [38;5;3m[KafkaProducerService] [39m[32mKafka producer connected[39m
intellidine-order-service         | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-order-service         | Please manually install OpenSSL and try installing Prisma again.
intellidine-order-service         | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-order-service         | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-order-service         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-order-service         | 
intellidine-order-service         | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-order-service         | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-order-service         | 
intellidine-order-service         | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-order-service         |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-order-service         |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-order-service         |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-order-service         |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-order-service         |     at async Proxy.onModuleInit (/app/dist/src/prisma.service.js:14:9)
intellidine-order-service         |     at async Promise.all (index 1)
intellidine-order-service         |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-order-service         |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-order-service         |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-order-service         |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-order-service         |   clientVersion: '5.22.0',
intellidine-order-service         |   errorCode: undefined
intellidine-order-service         | }
intellidine-order-service         | 
intellidine-order-service         | Node.js v20.19.5
[Kintellidine-order-service exited with code 1 (restarting)
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +39ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +10ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +4ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/request-otp, POST} route[39m[38;5;3m +0ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/verify-otp, POST} route[39m[38;5;3m +1ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/staff/login, POST} route[39m[38;5;3m +1ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +3ms[39m
intellidine-auth-service          | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-auth-service          | Please manually install OpenSSL and try installing Prisma again.
intellidine-auth-service          | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-auth-service          | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-auth-service          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-auth-service          | 
intellidine-auth-service          | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-auth-service          | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-auth-service          | 
intellidine-auth-service          | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-auth-service          |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-auth-service          |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-auth-service          |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599) {
intellidine-auth-service          |   clientVersion: '5.22.0',
intellidine-auth-service          |   errorCode: undefined
intellidine-auth-service          | }
intellidine-auth-service          | 
intellidine-auth-service          | Node.js v20.19.5
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +50ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +7ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +4ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu, GET} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/categories, GET} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, GET} route[39m[38;5;3m +0ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items, POST} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, PATCH} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, DELETE} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:27:56 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +4ms[39m
intellidine-menu-service          | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-menu-service          | Please manually install OpenSSL and try installing Prisma again.
intellidine-menu-service          | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-menu-service          | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-menu-service          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-menu-service          | 
intellidine-menu-service          | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-menu-service          | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-menu-service          | 
intellidine-menu-service          | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-menu-service          |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-menu-service          |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-menu-service          |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599) {
intellidine-menu-service          |   clientVersion: '5.22.0',
intellidine-menu-service          |   errorCode: undefined
intellidine-menu-service          | }
intellidine-menu-service          | 
intellidine-menu-service          | Node.js v20.19.5
[Kintellidine-auth-service exited with code 1 (restarting)
[Kintellidine-menu-service exited with code 1 (restarting)
intellidine-grafana               | logger=infra.usagestats t=2025-10-21T17:27:59.77439385Z level=info msg="Usage stats are ready to report"
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:00 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:00 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +16ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:00 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/inventory}:[39m[38;5;3m +5ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:00 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/health, GET} route[39m[38;5;3m +2ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:00 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, POST} route[39m[38;5;3m +1ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:00 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, GET} route[39m[38;5;3m +1ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:00 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items, GET} route[39m[38;5;3m +0ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:00 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/items/:id, PATCH} route[39m[38;5;3m +1ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:00 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/deduct, PATCH} route[39m[38;5;3m +1ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:00 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/alerts, GET} route[39m[38;5;3m +1ms[39m
intellidine-inventory-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:00 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/inventory/stats, GET} route[39m[38;5;3m +0ms[39m
intellidine-inventory-service     | {"level":"INFO","timestamp":"2025-10-21T17:28:00.572Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"inventory-group"}
intellidine-inventory-service     | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-inventory-service     | Please manually install OpenSSL and try installing Prisma again.
intellidine-inventory-service     | [31m[Nest] 1  - [39m10/21/2025, 5:28:00 PM [31m  ERROR[39m [38;5;3m[InventoryService] [39m[31mâ Failed to start Inventory Service:[39m
intellidine-inventory-service     | [31m[Nest] 1  - [39m10/21/2025, 5:28:00 PM [31m  ERROR[39m [38;5;3m[InventoryService] [39m[31mPrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-inventory-service     | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-inventory-service     | 
intellidine-inventory-service     | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)[39m
intellidine-kafka                 | [2025-10-21 17:28:00,592] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group inventory-group in Empty state. Created a new member id inventory-service-6ee0f4a5-2a92-45b1-ab79-93d58f37bbdc and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[Kintellidine-inventory-service exited with code 1 (restarting)
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:28:01 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:28:01 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +17ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:28:01 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/payments}:[39m[38;5;3m +8ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:28:01 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/create-razorpay-order, POST} route[39m[38;5;3m +4ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:28:01 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/verify-razorpay, POST} route[39m[38;5;3m +1ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:28:01 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/confirm-cash, POST} route[39m[38;5;3m +0ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:28:01 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/health, GET} route[39m[38;5;3m +1ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:28:01 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/stats/daily, GET} route[39m[38;5;3m +0ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:28:01 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments/:payment_id, GET} route[39m[38;5;3m +1ms[39m
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:28:01 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/payments, GET} route[39m[38;5;3m +0ms[39m
intellidine-payment-service       | {"level":"WARN","timestamp":"2025-10-21T17:28:01.988Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
intellidine-payment-service       | [32m[Nest] 1  - [39m10/21/2025, 5:28:02 PM [32m    LOG[39m [38;5;3m[PaymentProducer] [39m[32mâ Kafka Producer (Payment Service) connected[39m
intellidine-payment-service       | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-payment-service       | Please manually install OpenSSL and try installing Prisma again.
intellidine-payment-service       | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-payment-service       | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-payment-service       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-payment-service       | 
intellidine-payment-service       | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-payment-service       | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-payment-service       | 
intellidine-payment-service       | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-payment-service       |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-payment-service       |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-payment-service       |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-payment-service       |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-payment-service       |     at async Proxy.onModuleInit (/app/dist/src/prisma.service.js:14:9)
intellidine-payment-service       |     at async Promise.all (index 1)
intellidine-payment-service       |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-payment-service       |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-payment-service       |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-payment-service       |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-payment-service       |   clientVersion: '5.22.0',
intellidine-payment-service       |   errorCode: undefined
intellidine-payment-service       | }
intellidine-payment-service       | 
intellidine-payment-service       | Node.js v20.19.5
[Kintellidine-payment-service exited with code 1 (restarting)
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:28:02 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:28:02 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +17ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:28:02 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +6ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:28:02 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +2ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:28:02 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, POST} route[39m[38;5;3m +1ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:28:02 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id, GET} route[39m[38;5;3m +1ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:28:02 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders, GET} route[39m[38;5;3m +0ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:28:02 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/status, PATCH} route[39m[38;5;3m +2ms[39m
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:28:02 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/orders/:id/cancel, PATCH} route[39m[38;5;3m +1ms[39m
intellidine-order-service         | {"level":"WARN","timestamp":"2025-10-21T17:28:02.677Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
intellidine-order-service         | [32m[Nest] 1  - [39m10/21/2025, 5:28:02 PM [32m    LOG[39m [38;5;3m[KafkaProducerService] [39m[32mKafka producer connected[39m
intellidine-order-service         | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-order-service         | Please manually install OpenSSL and try installing Prisma again.
intellidine-order-service         | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-order-service         | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-order-service         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-order-service         | 
intellidine-order-service         | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-order-service         | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-order-service         | 
intellidine-order-service         | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-order-service         |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-order-service         |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-order-service         |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-order-service         |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-order-service         |     at async Proxy.onModuleInit (/app/dist/src/prisma.service.js:14:9)
intellidine-order-service         |     at async Promise.all (index 1)
intellidine-order-service         |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-order-service         |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-order-service         |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-order-service         |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-order-service         |   clientVersion: '5.22.0',
intellidine-order-service         |   errorCode: undefined
intellidine-order-service         | }
intellidine-order-service         | 
intellidine-order-service         | Node.js v20.19.5
[Kintellidine-order-service exited with code 1 (restarting)
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:03 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:03 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +26ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:03 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/api/analytics}:[39m[38;5;3m +12ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:03 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/health, GET} route[39m[38;5;3m +10ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:03 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/daily, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:03 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/recent, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:03 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/range, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:03 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/metrics/aggregated, GET} route[39m[38;5;3m +0ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:03 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/analytics/orders/history, GET} route[39m[38;5;3m +1ms[39m
intellidine-analytics-service     | [32m[Nest] 1  - [39m10/21/2025, 5:28:03 PM [32m    LOG[39m [38;5;3m[KafkaConsumerService] [39m[32mKafka consumer connected[39m
intellidine-analytics-service     | {"level":"INFO","timestamp":"2025-10-21T17:28:03.323Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"analytics-service-group"}
intellidine-kafka                 | [2025-10-21 17:28:03,339] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group analytics-service-group in Empty state. Created a new member id analytics-service-3fe6ed6e-86d0-445d-841a-c78260730a7d and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
intellidine-analytics-service     | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-analytics-service     | Please manually install OpenSSL and try installing Prisma again.
intellidine-analytics-service     | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-analytics-service     | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-analytics-service     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-analytics-service     | 
intellidine-analytics-service     | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-analytics-service     | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-analytics-service     | 
intellidine-analytics-service     | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-analytics-service     |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-analytics-service     |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-analytics-service     |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599)
intellidine-analytics-service     |     at async _r.start (/app/node_modules/@prisma/client/runtime/library.js:112:1976)
intellidine-analytics-service     |     at async Proxy.onModuleInit (/app/dist/prisma.service.js:19:9)
intellidine-analytics-service     |     at async Promise.all (index 0)
intellidine-analytics-service     |     at async callModuleInitHook (/app/node_modules/@nestjs/core/hooks/on-module-init.hook.js:43:5)
intellidine-analytics-service     |     at async NestApplication.callInitHook (/app/node_modules/@nestjs/core/nest-application-context.js:234:13)
intellidine-analytics-service     |     at async NestApplication.init (/app/node_modules/@nestjs/core/nest-application.js:100:9)
intellidine-analytics-service     |     at async NestApplication.listen (/app/node_modules/@nestjs/core/nest-application.js:170:13) {
intellidine-analytics-service     |   clientVersion: '5.22.0',
intellidine-analytics-service     |   errorCode: undefined
intellidine-analytics-service     | }
intellidine-analytics-service     | 
intellidine-analytics-service     | Node.js v20.19.5
[Kintellidine-analytics-service exited with code 1 (restarting)
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +43ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +11ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +5ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/request-otp, POST} route[39m[38;5;3m +1ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/customer/verify-otp, POST} route[39m[38;5;3m +0ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/auth/staff/login, POST} route[39m[38;5;3m +1ms[39m
intellidine-auth-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +3ms[39m
intellidine-auth-service          | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-auth-service          | Please manually install OpenSSL and try installing Prisma again.
intellidine-auth-service          | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-auth-service          | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-auth-service          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-auth-service          | 
intellidine-auth-service          | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-auth-service          | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-auth-service          | 
intellidine-auth-service          | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-auth-service          |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-auth-service          |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-auth-service          |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599) {
intellidine-auth-service          |   clientVersion: '5.22.0',
intellidine-auth-service          |   errorCode: undefined
intellidine-auth-service          | }
intellidine-auth-service          | 
intellidine-auth-service          | Node.js v20.19.5
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[NestFactory] [39m[32mStarting Nest application...[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[InstanceLoader] [39m[32mAppModule dependencies initialized[39m[38;5;3m +54ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[RoutesResolver] [39m[32mAppController {/}:[39m[38;5;3m +8ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/health, GET} route[39m[38;5;3m +5ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu, GET} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/categories, GET} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, GET} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items, POST} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, PATCH} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[RouterExplorer] [39m[32mMapped {/api/menu/items/:id, DELETE} route[39m[38;5;3m +1ms[39m
intellidine-menu-service          | [32m[Nest] 1  - [39m10/21/2025, 5:28:04 PM [32m    LOG[39m [38;5;3m[NestApplication] [39m[32mNest application successfully started[39m[38;5;3m +4ms[39m
intellidine-menu-service          | prisma:warn Prisma failed to detect the libssl/openssl version to use, and may not work as expected. Defaulting to "openssl-1.1.x".
intellidine-menu-service          | Please manually install OpenSSL and try installing Prisma again.
intellidine-menu-service          | /app/node_modules/@prisma/client/runtime/library.js:111
intellidine-menu-service          | ${r}`,A(n,!0));this.name="RequestError";this.code="P5010"}};w(kr,"RequestError");async function ct(e,t,r=n=>n){let n=t.clientVersion;try{return typeof fetch=="function"?await r(fetch)(e,t):await r(to)(e,t)}catch(i){let o=i.message??"Unknown error";throw new kr(o,{clientVersion:n})}}function im(e){return{...e.headers,"Content-Type":"application/json"}}function om(e){return{method:e.method,headers:im(e)}}function sm(e,t){return{text:()=>Promise.resolve(Buffer.concat(e).toString()),json:()=>Promise.resolve().then(()=>JSON.parse(Buffer.concat(e).toString())),ok:t.statusCode>=200&&t.statusCode<=299,status:t.statusCode,url:t.url,headers:new ro(t.headers)}}async function to(e,t={}){let r=am("https"),n=om(t),i=[],{origin:o}=new URL(e);return new Promise((s,a)=>{let l=r.request(e,n,u=>{let{statusCode:c,headers:{location:p}}=u;c>=301&&c<=399&&p&&(p.startsWith("http")===!1?s(to(`${o}${p}`,t)):s(to(p,t))),u.on("data",d=>i.push(d)),u.on("end",()=>s(sm(i,u))),u.on("error",a)});l.on("error",a),l.end(t.body??"")})}var am=typeof require<"u"?require:()=>{},ro=class{constructor(t={}){this.headers=new Map;for(let[r,n]of Object.entries(t))if(typeof n=="string")this.headers.set(r,n);else if(Array.isArray(n))for(let i of n)this.headers.set(r,i)}append(t,r){this.headers.set(t,r)}delete(t){this.headers.delete(t)}get(t){return this.headers.get(t)??null}has(t){return this.headers.has(t)}set(t,r){this.headers.set(t,r)}forEach(t,r){for(let[n,i]of this.headers)t.call(r,i,n,this)}};var lm=/^[1-9][0-9]*\.[0-9]+\.[0-9]+$/,ll=L("prisma:client:dataproxyEngine");async function um(e,t){let r=al["@prisma/engines-version"],n=t.clientVersion??"unknown";if(process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION)return process.env.PRISMA_CLIENT_DATA_PROXY_CLIENT_VERSION;if(e.includes("accelerate")&&n!=="0.0.0"&&n!=="in-memory")return n;let[i,o]=n?.split("-")??[];if(o===void 0&&lm.test(i))return i;if(o!==void 0||n==="0.0.0"||n==="in-memory"){if(e.startsWith("localhost")||e.startsWith("127.0.0.1"))return"0.0.0";let[s]=r.split("-")??[],[a,l,u]=s.split("."),c=cm(`<=${a}.${l}.${u}`),p=await ct(c,{clientVersion:n});if(!p.ok)throw new Error(`Failed to fetch stable Prisma version, unpkg.com status ${p.status} ${p.statusText}, response body: ${await p.text()||"<empty body>"}`);let d=await p.text();ll("length of body fetched from unpkg.com",d.length);let f;try{f=JSON.parse(d)}catch(g){throw console.error("JSON.parse error: body fetched from unpkg.com: ",d),g}return f.version}throw new lt("Only `major.minor.patch` versions are supported by Accelerate.",{clientVersion:n})}async function ul(e,t){let r=await um(e,t);return ll("version",r),r}function cm(e){return encodeURI(`https://unpkg.com/prisma@${e}/package.json`)}var cl=3,no=L("prisma:client:dataproxyEngine"),io=class{constructor({apiKey:t,tracingHelper:r,logLevel:n,logQueries:i,engineHash:o}){this.apiKey=t,this.tracingHelper=r,this.logLevel=n,this.logQueries=i,this.engineHash=o}build({traceparent:t,interactiveTransaction:r}={}){let n={Authorization:`Bearer ${this.apiKey}`,"Prisma-Engine-Hash":this.engineHash};this.tracingHelper.isEnabled()&&(n.traceparent=t??this.tracingHelper.getTraceParent()),r&&(n["X-transaction-id"]=r.id);let i=this.buildCaptureSettings();return i.length>0&&(n["X-capture-telemetry"]=i.join(", ")),n}buildCaptureSettings(){let t=[];return this.tracingHelper.isEnabled()&&t.push("tracing"),this.logLevel&&t.push(this.logLevel),this.logQueries&&t.push("query"),t}},Dr=class{constructor(t){this.name="DataProxyEngine";ol(t),this.config=t,this.env={...t.env,...typeof process<"u"?process.env:{}},this.inlineSchema=il(t.inlineSchema),this.inlineDatasources=t.inlineDatasources,this.inlineSchemaHash=t.inlineSchemaHash,this.clientVersion=t.clientVersion,this.engineHash=t.engineVersion,this.logEmitter=t.logEmitter,this.tracingHelper=t.tracingHelper}apiKey(){return this.headerBuilder.apiKey}version(){return this.engineHash}async start(){this.startPromise!==void 0&&await this.startPromise,this.startPromise=(async()=>{let[t,r]=this.extractHostAndApiKey();this.host=t,this.headerBuilder=new io({apiKey:r,tracingHelper:this.tracingHelper,logLevel:this.config.logLevel,logQueries:this.config.logQueries,engineHash:this.engineHash}),this.remoteClientVersion=await ul(t,this.config),no("host",this.host)})(),await this.startPromise}async stop(){}propagateResponseExtensions(t){t?.logs?.length&&t.logs.forEach(r=>{switch(r.level){case"debug":case"error":case"trace":case"warn":case"info":break;case"query":{let n=typeof r.attributes.query=="string"?r.attributes.query:"";if(!this.tracingHelper.isEnabled()){let[i]=n.split("/* traceparent");n=i}this.logEmitter.emit("query",{query:n,timestamp:sl(r.timestamp),duration:Number(r.attributes.duration_ms),params:r.attributes.params,target:r.attributes.target})}}}),t?.traces?.length&&this.tracingHelper.createEngineSpan({span:!0,spans:t.traces})}onBeforeExit(){throw new Error('"beforeExit" hook is not applicable to the remote query engine')}async url(t){return await this.start(),`https://${this.host}/${this.remoteClientVersion}/${this.inlineSchemaHash}/${t}`}async uploadSchema(){let t={name:"schemaUpload",internal:!0};return this.tracingHelper.runInChildSpan(t,async()=>{let r=await ct(await this.url("schema"),{method:"PUT",headers:this.headerBuilder.build(),body:this.inlineSchema,clientVersion:this.clientVersion});r.ok||no("schema response status",r.status);let n=await Or(r,this.clientVersion);if(n)throw this.logEmitter.emit("warn",{message:`Error while uploading schema: ${n.message}`,timestamp:new Date,target:""}),n;this.logEmitter.emit("info",{message:`Schema (re)uploaded (hash: ${this.inlineSchemaHash})`,timestamp:new Date,target:""})})}request(t,{traceparent:r,interactiveTransaction:n,customDataProxyFetch:i}){return this.requestInternal({body:t,traceparent:r,interactiveTransaction:n,customDataProxyFetch:i})}async requestBatch(t,{traceparent:r,transaction:n,customDataProxyFetch:i}){let o=n?.kind==="itx"?n.options:void 0,s=Ft(t,n),{batchResult:a,elapsed:l}=await this.requestInternal({body:s,customDataProxyFetch:i,interactiveTransaction:o,traceparent:r});return a.map(u=>"errors"in u&&u.errors.length>0?st(u.errors[0],this.clientVersion,this.config.activeProvider):{data:u,elapsed:l})}requestInternal({body:t,traceparent:r,customDataProxyFetch:n,interactiveTransaction:i}){return this.withRetry({actionGerund:"querying",callback:async({logHttpCall:o})=>{let s=i?`${i.payload.endpoint}/graphql`:await this.url("graphql");o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r,interactiveTransaction:i}),body:JSON.stringify(t),clientVersion:this.clientVersion},n);a.ok||no("graphql response status",a.status),await this.handleError(await Or(a,this.clientVersion));let l=await a.json(),u=l.extensions;if(u&&this.propagateResponseExtensions(u),l.errors)throw l.errors.length===1?st(l.errors[0],this.config.clientVersion,this.config.activeProvider):new B(l.errors,{clientVersion:this.config.clientVersion});return l}})}async transaction(t,r,n){let i={start:"starting",commit:"committing",rollback:"rolling back"};return this.withRetry({actionGerund:`${i[t]} transaction`,callback:async({logHttpCall:o})=>{if(t==="start"){let s=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel}),a=await this.url("transaction/start");o(a);let l=await ct(a,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),body:s,clientVersion:this.clientVersion});await this.handleError(await Or(l,this.clientVersion));let u=await l.json(),c=u.extensions;c&&this.propagateResponseExtensions(c);let p=u.id,d=u["data-proxy"].endpoint;return{id:p,payload:{endpoint:d}}}else{let s=`${n.payload.endpoint}/${t}`;o(s);let a=await ct(s,{method:"POST",headers:this.headerBuilder.build({traceparent:r.traceparent}),clientVersion:this.clientVersion});await this.handleError(await Or(a,this.clientVersion));let u=(await a.json()).extensions;u&&this.propagateResponseExtensions(u);return}}})}extractHostAndApiKey(){let t={clientVersion:this.clientVersion},r=Object.keys(this.inlineDatasources)[0],n=Nt({inlineDatasources:this.inlineDatasources,overrideDatasources:this.config.overrideDatasources,clientVersion:this.clientVersion,env:this.env}),i;try{i=new URL(n)}catch{throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t)}let{protocol:o,host:s,searchParams:a}=i;if(o!=="prisma:"&&o!=="prisma+postgres:")throw new at(`Error validating datasource \`${r}\`: the URL must start with the protocol \`prisma://\``,t);let l=a.get("api_key");if(l===null||l.length<1)throw new at(`Error validating datasource \`${r}\`: the URL must contain a valid API key`,t);return[s,l]}metrics(){throw new lt("Metrics are not yet supported for Accelerate",{clientVersion:this.clientVersion})}async withRetry(t){for(let r=0;;r++){let n=i=>{this.logEmitter.emit("info",{message:`Calling ${i} (n=${r})`,timestamp:new Date,target:""})};try{return await t.callback({logHttpCall:n})}catch(i){if(!(i instanceof se)||!i.isRetryable)throw i;if(r>=cl)throw i instanceof Mt?i.cause:i;this.logEmitter.emit("warn",{message:`Attempt ${r+1}/${cl} failed for ${t.actionGerund}: ${i.message??"(unknown)"}`,timestamp:new Date,target:""});let o=await nl(r);this.logEmitter.emit("warn",{message:`Retrying after ${o}ms`,timestamp:new Date,target:""})}}}async handleError(t){if(t instanceof ut)throw await this.uploadSchema(),new Mt({clientVersion:this.clientVersion,cause:t});if(t)throw t}applyPendingMigrations(){throw new Error("Method not implemented.")}};function pl(e){if(e?.kind==="itx")return e.options.id}var so=k(require("os")),dl=k(require("path"));var oo=Symbol("PrismaLibraryEngineCache");function pm(){let e=globalThis;return e[oo]===void 0&&(e[oo]={}),e[oo]}function dm(e){let t=pm();if(t[e]!==void 0)return t[e];let r=dl.default.toNamespacedPath(e),n={exports:{}},i=0;return process.platform!=="win32"&&(i=so.default.constants.dlopen.RTLD_LAZY|so.default.constants.dlopen.RTLD_DEEPBIND),process.dlopen(n,r,i),t[e]=n.exports,n.exports}var ml={async loadLibrary(e){let t=await Yn(),r=await za("library",e);try{return e.tracingHelper.runInChildSpan({name:"loadLibrary",internal:!0},()=>dm(r))}catch(n){let i=ui({e:n,platformInfo:t,id:r});throw new R(i,e.clientVersion)}}};var ao,fl={async loadLibrary(e){let{clientVersion:t,adapter:r,engineWasm:n}=e;if(r===void 0)throw new R(`The \`adapter\` option for \`PrismaClient\` is required in this context (${In().prettyName})`,t);if(n===void 0)throw new R("WASM engine was unexpectedly `undefined`",t);ao===void 0&&(ao=(async()=>{let o=n.getRuntime(),s=await n.getQueryEngineWasmModule();if(s==null)throw new R("The loaded wasm module was unexpectedly `undefined` or `null` once loaded",t);let a={"./query_engine_bg.js":o},l=new WebAssembly.Instance(s,a);return o.__wbg_set_wasm(l.exports),o.QueryEngine})());let i=await ao;return{debugPanic(){return Promise.reject("{}")},dmmf(){return Promise.resolve("{}")},version(){return{commit:"unknown",version:"unknown"}},QueryEngine:i}}};var mm="P2036",Ae=L("prisma:client:libraryEngine");function fm(e){return e.item_type==="query"&&"query"in e}function gm(e){return"level"in e?e.level==="error"&&e.message==="PANIC":!1}var gl=[...Jn,"native"],_r=class{constructor(t,r){this.name="LibraryEngine";this.libraryLoader=r??ml,t.engineWasm!==void 0&&(this.libraryLoader=r??fl),this.config=t,this.libraryStarted=!1,this.logQueries=t.logQueries??!1,this.logLevel=t.logLevel??"error",this.logEmitter=t.logEmitter,this.datamodel=t.inlineSchema,t.enableDebugLogs&&(this.logLevel="debug");let n=Object.keys(t.overrideDatasources)[0],i=t.overrideDatasources[n]?.url;n!==void 0&&i!==void 0&&(this.datasourceOverrides={[n]:i}),this.libraryInstantiationPromise=this.instantiateLibrary()}async applyPendingMigrations(){throw new Error("Cannot call this method from this type of engine instance")}async transaction(t,r,n){await this.start();let i=JSON.stringify(r),o;if(t==="start"){let a=JSON.stringify({max_wait:n.maxWait,timeout:n.timeout,isolation_level:n.isolationLevel});o=await this.engine?.startTransaction(a,i)}else t==="commit"?o=await this.engine?.commitTransaction(n.id,i):t==="rollback"&&(o=await this.engine?.rollbackTransaction(n.id,i));let s=this.parseEngineResponse(o);if(hm(s)){let a=this.getExternalAdapterError(s);throw a?a.error:new V(s.message,{code:s.error_code,clientVersion:this.config.clientVersion,meta:s.meta})}return s}async instantiateLibrary(){if(Ae("internalSetup"),this.libraryInstantiationPromise)return this.libraryInstantiationPromise;Qn(),this.binaryTarget=await this.getCurrentBinaryTarget(),await this.loadEngine(),this.version()}async getCurrentBinaryTarget(){{if(this.binaryTarget)return this.binaryTarget;let t=await nt();if(!gl.includes(t))throw new R(`Unknown ${ce("PRISMA_QUERY_ENGINE_LIBRARY")} ${ce(H(t))}. Possible binaryTargets: ${qe(gl.join(", "))} or a path to the query engine library.
intellidine-menu-service          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
intellidine-menu-service          | 
intellidine-menu-service          | PrismaClientInitializationError: Unable to require(`/app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node`).
intellidine-menu-service          | The Prisma engines do not seem to be compatible with your system. Please refer to the documentation about Prisma's system requirements: https://pris.ly/d/system-requirements
intellidine-menu-service          | 
intellidine-menu-service          | Details: Error loading shared library libssl.so.1.1: No such file or directory (needed by /app/node_modules/.prisma/client/libquery_engine-linux-musl.so.node)
intellidine-menu-service          |     at Object.loadLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:10243)
intellidine-menu-service          |     at async _r.loadEngine (/app/node_modules/@prisma/client/runtime/library.js:112:448)
intellidine-menu-service          |     at async _r.instantiateLibrary (/app/node_modules/@prisma/client/runtime/library.js:111:12599) {
intellidine-menu-service          |   clientVersion: '5.22.0',
intellidine-menu-service          |   errorCode: undefined
intellidine-menu-service          | }
intellidine-menu-service          | 
intellidine-menu-service          | Node.js v20.19.5
[Kintellidine-auth-service exited with code 1 (restarting)
[Kintellidine-menu-service exited with code 1 (restarting)
intellidine-api-gateway           | node:internal/modules/cjs/loader:1210
intellidine-api-gateway           |   throw err;
intellidine-api-gateway           |   ^
intellidine-api-gateway           | 
intellidine-api-gateway           | Error: Cannot find module 'axios'
intellidine-api-gateway           | Require stack:
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/http.module.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/dist/index.js
intellidine-api-gateway           | - /app/node_modules/@nestjs/axios/index.js
intellidine-api-gateway           | - /app/dist/src/app.module.js
intellidine-api-gateway           | - /app/dist/src/main.js
intellidine-api-gateway           |     at Module._resolveFilename (node:internal/modules/cjs/loader:1207:15)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1038:27)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19)
intellidine-api-gateway           |     at require (node:internal/modules/helpers:182:18)
intellidine-api-gateway           |     at Object.<anonymous> (/app/node_modules/@nestjs/axios/dist/http.module.js:16:33)
intellidine-api-gateway           |     at Module._compile (node:internal/modules/cjs/loader:1521:14)
intellidine-api-gateway           |     at Module._extensions..js (node:internal/modules/cjs/loader:1623:10)
intellidine-api-gateway           |     at Module.load (node:internal/modules/cjs/loader:1266:32)
intellidine-api-gateway           |     at Module._load (node:internal/modules/cjs/loader:1091:12)
intellidine-api-gateway           |     at Module.require (node:internal/modules/cjs/loader:1289:19) {
intellidine-api-gateway           |   code: 'MODULE_NOT_FOUND',
intellidine-api-gateway           |   requireStack: [
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/http.module.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/dist/index.js',
intellidine-api-gateway           |     '/app/node_modules/@nestjs/axios/index.js',
intellidine-api-gateway           |     '/app/dist/src/app.module.js',
intellidine-api-gateway           |     '/app/dist/src/main.js'
intellidine-api-gateway           |   ]
intellidine-api-gateway           | }
intellidine-api-gateway           | 
intellidine-api-gateway           | Node.js v20.19.5
[Kintellidine-api-gateway exited with code 1 (restarting)
Gracefully Stopping... press Ctrl+C again to force
[K Container intellidine-prometheus  [32mStopping[0m
[K Container intellidine-analytics-service  [32mStopping[0m
[K Container intellidine-notification-service  [32mStopping[0m
[K Container intellidine-menu-service  [32mStopping[0m
[K Container intellidine-ml-service  [32mStopping[0m
[K Container intellidine-discount-engine  [32mStopping[0m
[K Container intellidine-order-service  [32mStopping[0m
[K Container intellidine-inventory-service  [32mStopping[0m
[K Container intellidine-nginx  [32mStopping[0m
[K Container intellidine-grafana  [32mStopping[0m
[K Container intellidine-auth-service  [32mStopping[0m
[K Container intellidine-payment-service  [32mStopping[0m
[K Container intellidine-payment-service  [32mStopped[0m
[K Container intellidine-analytics-service  [32mStopped[0m
[K Container intellidine-menu-service  [32mStopped[0m
[K Container intellidine-inventory-service  [32mStopped[0m
[K Container intellidine-auth-service  [32mStopped[0m
[K Container intellidine-order-service  [32mStopped[0m
[K Container intellidine-prometheus  [32mStopped[0m
[K Container intellidine-grafana  [32mStopped[0m
[K Container intellidine-nginx  [32mStopped[0m
[K Container intellidine-api-gateway  [32mStopping[0m
[K Container intellidine-api-gateway  [32mStopped[0m
[K Container intellidine-redis  [32mStopping[0m
[K Container intellidine-postgres  [32mStopping[0m
[K Container intellidine-ml-service  [32mStopped[0m
[K Container intellidine-postgres  [32mStopped[0m
[K Container intellidine-redis  [32mStopped[0m
